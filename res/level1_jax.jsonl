{"id": "5d8c7463-1d18-48e5-991f-dc35448a034a", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(A, B):\n    \"\"\"\n    Simple model that performs matrix-vector multiplication (C = A * B).\n    \"\"\"\n    return jnp.matmul(A, B)\n\nM = 256 * 8 # 2048\nK = 131072 * 8 # 1048576\n\ndef get_inputs(key):\n    \"\"\"\n    Generates random input matrix and vector.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the input matrix and vector [A, B].\n    \"\"\"\n    key_a, key_b = jax.random.split(key)\n    A = jax.random.normal(key_a, (M, K))\n    B = jax.random.normal(key_b, (K, 1))\n    return [A, B]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []\n", "description": "This code implements 4 Matrix vector multiplication .\n\nSimple model that performs matrix-vector multiplication (C = A * B).", "difficulty": 3}
{"id": "8cbe5a8b-7f62-4c68-b4b1-4f87e77e65f1", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(A, B):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B) with a small K dimension\n    \"\"\"\n    return jnp.matmul(A, B)\n\nM = 16384 * 2\nN = 16384 * 2\nK = 32 * 2\n\ndef get_inputs(key):\n    \"\"\"\n    Generates random input matrices A and B.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the two input matrices [A, B].\n    \"\"\"\n    key_a, key_b = jax.random.split(key)\n    A = jax.random.normal(key_a, (M, K))\n    B = jax.random.normal(key_b, (K, N))\n    return [A, B]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []\n", "description": "This code implements 7 Matmul with small K dimension .\n\nSimple model that performs a single matrix multiplication (C = A * B) with a small K dimension", "difficulty": 3}
{"id": "727f4725-6843-4658-a87c-e0c84a3d2b1c", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(A, B):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B)\n    \"\"\"\n    return jnp.matmul(A.T, B)\n\nM = 1024 * 2\nK = 4096 * 2\nN = 2048 * 2\n\ndef get_inputs(key):\n    \"\"\"\n    Generates random input matrices A and B.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the two input matrices [A, B].\n    \"\"\"\n    key_a, key_b = jax.random.split(key)\n    A = jax.random.normal(key_a, (K, M))\n    B = jax.random.normal(key_b, (K, N))\n    return [A, B]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []\n", "description": "This code implements 16 Matmul with transposed A.\n\nSimple model that performs a single matrix multiplication (C = A * B)", "difficulty": 3}
{"id": "c8aee113-ca0c-403e-9694-aa7a60da8e2b", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nkernel_size = 3\nstride = 2\npadding = 1\ndilation = 3\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        # PyTorch input: (N, C, D, H, W)\n        # JAX input: (N, D, H, W, C)\n        # We will generate data in JAX format.\n        return nn.max_pool(x,\n                           window_shape=(kernel_size, kernel_size, kernel_size),\n                           strides=(stride, stride, stride),\n                           padding=[(padding, padding), (padding, padding), (padding, padding)],\n                           window_dilation=(dilation, dilation, dilation))\n\nbatch_size = 16\nchannels = 32\ndim1 = 128\ndim2 = 128\ndim3 = 128\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim1, dim2, dim3, channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)\n", "description": "This code implements 43 Max Pooling 3D.\n\nGenerates a random input tensor.", "difficulty": 3}
{"id": "0f2f65e8-39e2-4840-8c91-dd5231f11a26", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nin_channels = 64\nout_channels = 64\nkernel_size = (3, 7)\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        # PyTorch input: (N, C, H, W)\n        # JAX input: (N, H, W, C)\n        # We will generate data in JAX format.\n        return nn.ConvTranspose(features=out_channels, kernel_size=kernel_size, use_bias=False)(x)\n\n# Test code\nbatch_size = 8\nwidth = 512\nheight = 512\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, height, width, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)\n", "description": "This code implements 65 conv transposed 2D  square input  asymmetric kernel.\n\nGenerates a random input tensor.", "difficulty": 3}
{"id": "4568f2de-3667-411d-beb9-6f5777f1a19f", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nin_channels = 64\nout_channels = 128\nkernel_size = (5, 7)\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        # PyTorch input: (N, C, H, W)\n        # JAX input: (N, H, W, C)\n        # We will generate data in JAX format.\n        return nn.Conv(features=out_channels, kernel_size=kernel_size, use_bias=False)(x)\n\n# Test code\nbatch_size = 8\nheight = 512\nwidth = 256\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, height, width, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)\n", "description": "This code implements 56 conv standard 2D  asymmetric input  asymmetric kernel.\n\nGenerates a random input tensor.", "difficulty": 3}
{"id": "559c8749-b350-4308-8be7-b57659a74ca4", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nnum_groups = 8\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs Group Normalization.\n    \"\"\"\n    @nn.compact\n    def __call__(self, x):\n        \"\"\"\n        Applies Group Normalization to the input tensor.\n\n        Args:\n            x (jax.numpy.ndarray): Input tensor of shape (batch_size, dim1, dim2, features).\n\n        Returns:\n            jax.numpy.ndarray: Output tensor with Group Normalization applied, same shape as input.\n        \"\"\"\n        return nn.GroupNorm(num_groups=num_groups)(x)\n\nbatch_size = 112  # scaled up\nfeatures = 64\ndim1 = 512\ndim2 = 512\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim1, dim2, features))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)\n", "description": "This code implements 35 GroupNorm .\n\nSimple model that performs Group Normalization.", "difficulty": 3}
{"id": "ec4d3496-b3a1-4d8b-b8d4-939908c31341", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(x):\n    \"\"\"\n    Simple model that performs a Softmax activation.\n    \"\"\"\n    return nn.softmax(x, axis=1)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the input tensor [x].\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []\n", "description": "This code implements 23 Softmax.\n\nSimple model that performs a Softmax activation.", "difficulty": 3}
{"id": "a419c16f-0293-4af6-88c2-2c3e24cc2eab", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nreduce_dim = 1\n\ndef model(x):\n    \"\"\"\n    Simple model that finds the index of the minimum value along a specified dimension.\n    \"\"\"\n    return jnp.argmin(x, axis=reduce_dim)\n\nbatch_size = 128\ndim1 = 4096\ndim2 = 4095\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim1, dim2))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n    \"\"\"\n    return []\n", "description": "This code implements 52 Argmin over a dimension.\n\nSimple model that finds the index of the minimum value along a specified dimension.", "difficulty": 3}
{"id": "d4428536-95bc-4368-98fd-51b5b9648c80", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nin_channels = 3\nout_channels = 64\nkernel_size = 3\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        # PyTorch input: (N, C, D, H, W)\n        # JAX input: (N, D, H, W, C)\n        # We will generate data in JAX format.\n        return nn.Conv(features=out_channels, kernel_size=(kernel_size, kernel_size, kernel_size), use_bias=False)(x)\n\n# Test code\nbatch_size = 16\ndepth = 64\nwidth = 64\nheight = 64\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, depth, width, height, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)\n", "description": "This code implements 54 conv standard 3D  square input  square kernel.\n\nGenerates a random input tensor.", "difficulty": 3}
{"id": "76bb7203-4319-461a-9343-754cddf39afa", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(A, B):\n    \"\"\"\n    Performs 4D tensor-matrix multiplication: \n        C[b, i, j, k] = sum_l A[b, i, j, l] * B[l, k]\n\n    Args:\n        A (jax.numpy.ndarray): Input 4D tensor of shape (b, i, j, l)\n        B (jax.numpy.ndarray): Input matrix of shape (l, k)\n\n    Returns:\n        jax.numpy.ndarray: Output 4D tensor of shape (b, i, j, k)\n    \"\"\"\n    return jnp.einsum(\"bijl,lk->bijk\", A, B)\n\n# Test code\nb = 8\ni = 256\nj = 512\nl = 256\nk = 768\n\ndef get_inputs(key):\n    \"\"\"\n    Generates random input 4D tensor and matrix.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the input tensor and matrix [A, B].\n    \"\"\"\n    key_a, key_b = jax.random.split(key)\n    A = jax.random.normal(key_a, (b, i, j, l))\n    B = jax.random.normal(key_b, (l, k))\n    return [A, B]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []\n", "description": "This code implements 11 4D tensor matrix multiplication.\n\nPerforms 4D tensor-matrix multiplication: \n        C[b, i, j, k] = sum_l A[b, i, j, l] * B[l, k]\n\n    Args:\n        A (jax.numpy.ndarray): Input 4D tensor of shape (b, i, j, l)\n        B (jax.numpy.ndarray): Input matrix of shape (l, k)\n\n    Returns:\n        jax.numpy.ndarray: Output 4D tensor of shape (b, i, j, k)", "difficulty": 3}
{"id": "d1c3edaa-a291-48cd-badb-dddc5090a7e7", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nreduce_dim = 1\n\ndef model(x):\n    \"\"\"\n    Simple model that performs min reduction over a specific dimension.\n    \"\"\"\n    return jnp.min(x, axis=reduce_dim)\n\nbatch_size = 128\ndim1 = 4096\ndim2 = 4095\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim1, dim2))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n    \"\"\"\n    return []\n", "description": "This code implements 53 Min reduction over a dimension.\n\nSimple model that performs min reduction over a specific dimension.", "difficulty": 3}
{"id": "5db00e4b-0a10-45c2-92ab-1ccd0147ce1a", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(x):\n    \"\"\"\n    Simple model that performs L1 normalization.\n    \"\"\"\n    return x / jnp.mean(jnp.abs(x), axis=1, keepdims=True)\n\nbatch_size = 32768\n# choose dim so total <2^31\ndim = 65535\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n    \"\"\"\n    return []\n", "description": "This code implements 38 L1Norm .\n\nSimple model that performs L1 normalization.", "difficulty": 3}
{"id": "772c3649-0d45-4a0e-baa0-417b0d721d4b", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nin_channels = 32\nout_channels = 32\nkernel_size = (3, 7)\nstride = (1, 1)\npadding = (1, 3)\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        # PyTorch input: (N, C, H, W)\n        # JAX input: (N, H, W, C)\n        # We will generate data in JAX format.\n        return nn.ConvTranspose(features=out_channels,\n                                kernel_size=kernel_size,\n                                strides=stride,\n                                padding=[(padding[0], padding[0]), (padding[1], padding[1])],\n                                use_bias=False)(x)\n\n# Test code\nbatch_size = 8\nheight = 512\nwidth = 1024\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, height, width, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)\n", "description": "This code implements 78 conv transposed 2D asymmetric input asymmetric kernel   padded  .\n\nGenerates a random input tensor.", "difficulty": 3}
{"id": "8c0ec885-7887-418f-a758-8cbc21bfe4c2", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nin_channels = 32\nout_channels = 32\nkernel_size = 3\nstride = 2\npadding = 1\ngroups = 4\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        # PyTorch input: (N, C, D, H, W)\n        # JAX input: (N, D, H, W, C)\n        # We will generate data in JAX format.\n        return nn.ConvTranspose(features=out_channels,\n                                kernel_size=(kernel_size, kernel_size, kernel_size),\n                                strides=(stride, stride, stride),\n                                padding=[(padding, padding), (padding, padding), (padding, padding)],\n                                feature_group_count=groups,\n                                use_bias=False)(x)\n\n# Test code\nbatch_size = 4\ndepth = 32\nheight = 64\nwidth = 128\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, depth, height, width, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)\n", "description": "This code implements 73 conv transposed 3D asymmetric input square kernel  strided padded  grouped.\n\nGenerates a random input tensor.", "difficulty": 3}
{"id": "6c9eec50-3687-4f8f-9a88-e18d446fb7d3", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nin_channels = 32\nout_channels = 64\nkernel_depth = 3\nkernel_width = 5\nkernel_height = 5\nkernel_size = (kernel_depth, kernel_width, kernel_height)\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        # PyTorch input: (N, C, D, W, H)\n        # JAX input: (N, D, W, H, C)\n        # We will generate data in JAX format.\n        return nn.ConvTranspose(features=out_channels, kernel_size=kernel_size, use_bias=False)(x)\n\n# Test code\nbatch_size = 16\ndepth = 64\nwidth = 64\nheight = 64\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, depth, width, height, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)\n", "description": "This code implements 68 conv transposed 3D  square input  asymmetric kernel.\n\nGenerates a random input tensor.", "difficulty": 3}
{"id": "cb8a48e3-5cb1-4b0b-b7f1-33012b332d2f", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nkernel_size = 8\nstride      = 1\npadding     = 4\ndilation    = 3\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        # PyTorch input: (N, C, W)\n        # JAX input: (N, W, C)\n        # We will generate data in JAX format.\n        return nn.max_pool(x,\n                           window_shape=(kernel_size,),\n                           strides=(stride,),\n                           padding=[(padding, padding)],\n                           window_dilation=(dilation,))\n\nbatch_size = 64\nfeatures = 192\nsequence_length = 65536\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, sequence_length, features))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)\n", "description": "This code implements 41 Max Pooling 1D.\n\nGenerates a random input tensor.", "difficulty": 3}
{"id": "bb1fc4c0-119d-4a10-b9af-c56da868cbac", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nreduce_dim = 1\n\ndef model(x):\n    \"\"\"\n    Simple model that performs sum reduction over a specified dimension.\n    \"\"\"\n    return jnp.sum(x, axis=reduce_dim, keepdims=True)\n\nbatch_size = 128\ndim1 = 4096\ndim2 = 4095\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim1, dim2))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n    \"\"\"\n    return []\n", "description": "This code implements 47 Sum reduction over a dimension.\n\nSimple model that performs sum reduction over a specified dimension.", "difficulty": 3}
{"id": "3a0fb29e-56aa-48dc-b2c6-3a95bb8c9f4e", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs Instance Normalization.\n    \"\"\"\n    @nn.compact\n    def __call__(self, x):\n        \"\"\"\n        Applies Instance Normalization to the input tensor.\n\n        Args:\n            x (jax.numpy.ndarray): Input tensor of shape (batch_size, dim1, dim2, features).\n\n        Returns:\n            jax.numpy.ndarray: Output tensor with Instance Normalization applied, same shape as input.\n        \"\"\"\n        return nn.InstanceNorm()(x)\n\nbatch_size = 112  # heavier workload\nfeatures = 64\ndim1 = 512\ndim2 = 512\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim1, dim2, features))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)\n", "description": "This code implements 34 InstanceNorm.\n\nSimple model that performs Instance Normalization.", "difficulty": 3}
{"id": "1192b589-bc37-4804-8d16-6bb4293d2f1c", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nin_channels = 64\nout_channels = 128\nkernel_size = (3, 5)\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        # PyTorch input: (N, C, H, W)\n        # JAX input: (N, H, W, C)\n        # We will generate data in JAX format.\n        return nn.ConvTranspose(features=out_channels, kernel_size=kernel_size, use_bias=False)(x)\n\n# Test code\nbatch_size = 64\nheight_in = 128\nwidth_in = 256\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, height_in, width_in, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)\n", "description": "This code implements 69 conv transposed 2D  asymmetric input  asymmetric kernel.\n\nGenerates a random input tensor.", "difficulty": 3}
{"id": "95b8859d-2e92-4335-92fc-24611e8a90aa", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nkernel_size = 4\nstride = 1\npadding = 1\ndilation = 1\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        # PyTorch input: (N, C, H, W)\n        # JAX input: (N, H, W, C)\n        # We will generate data in JAX format.\n        return nn.max_pool(x,\n                           window_shape=(kernel_size, kernel_size),\n                           strides=(stride, stride),\n                           padding=[(padding, padding), (padding, padding)],\n                           window_dilation=(dilation, dilation))\n\nbatch_size = 32\nchannels = 64\nheight = 512\nwidth = 512\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, height, width, channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)\n", "description": "This code implements 42 Max Pooling 2D.\n\nGenerates a random input tensor.", "difficulty": 3}
{"id": "1c1e9146-5546-4978-bab6-fb112db149e3", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs RMS Normalization.\n    \"\"\"\n    @nn.compact\n    def __call__(self, x):\n        \"\"\"\n        Applies RMS Normalization to the input tensor.\n\n        Args:\n            x (jax.numpy.ndarray): Input tensor of shape (batch_size, dim1, dim2, features).\n\n        Returns:\n            jax.numpy.ndarray: Output tensor with RMS Normalization applied, same shape as input.\n        \"\"\"\n        return nn.RMSNorm()(x)\n\nbatch_size = 112\nfeatures = 64\ndim1 = 512\ndim2 = 512\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim1, dim2, features))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)\n", "description": "This code implements 36 RMSNorm .\n\nSimple model that performs RMS Normalization.", "difficulty": 3}
{"id": "fe0ded16-1d59-45ae-a1ca-e169ed6615e2", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(x):\n    \"\"\"\n    Simple model that performs Frobenius norm normalization.\n    \"\"\"\n    norm = jnp.linalg.norm(x, ord='fro')\n    return x / norm\n\nbatch_size = 112\nfeatures = 64\ndim1 = 512\ndim2 = 512\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, features, dim1, dim2))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n    \"\"\"\n    return []\n", "description": "This code implements 37 FrobeniusNorm .\n\nSimple model that performs Frobenius norm normalization.", "difficulty": 3}
{"id": "830bb310-9a3e-4d25-ba4c-761390738b7b", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nin_channels = 48\nout_channels = 24\nkernel_size = 3\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        # PyTorch input: (N, C, D, H, W)\n        # JAX input: (N, D, H, W, C)\n        # We will generate data in JAX format.\n        return nn.ConvTranspose(features=out_channels, kernel_size=(kernel_size, kernel_size, kernel_size), use_bias=False)(x)\n\n# Test code\nbatch_size = 8\ndepth = 96\nheight = 96\nwidth = 96\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, depth, height, width, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)\n", "description": "This code implements 70 conv transposed 3D  asymmetric input  square kernel.\n\nGenerates a random input tensor.", "difficulty": 3}
{"id": "25ae69ad-3a11-431d-93b5-233b89d75bef", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nkernel_size = 8\nstride = 1\npadding = 4\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        # PyTorch input: (N, C, W)\n        # JAX input: (N, W, C)\n        # We will generate data in JAX format.\n        return nn.avg_pool(x,\n                           window_shape=(kernel_size,),\n                           strides=(stride,),\n                           padding=[(padding, padding)])\n\nbatch_size = 64\nin_channels = 128\ninput_length = 65536\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, input_length, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)\n", "description": "This code implements 44 Average Pooling 1D.\n\nGenerates a random input tensor.", "difficulty": 3}
{"id": "d5393f62-dea2-4feb-8972-262d4668106e", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nreduce_dim = 1\n\ndef model(x):\n    \"\"\"\n    Simple model that performs Max reduction over a specific dimension.\n    \"\"\"\n    return jnp.max(x, axis=reduce_dim)\n\nbatch_size = 128\ndim1 = 4096\ndim2 = 4095\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim1, dim2))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n    \"\"\"\n    return []\n", "description": "This code implements 49 Max reduction over a dimension.\n\nSimple model that performs Max reduction over a specific dimension.", "difficulty": 3}
{"id": "0ebae6a9-f3d3-41d6-8cf5-0e02768c8e23", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nkernel_size = 3\nstride = 2\npadding = 1\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        # PyTorch input: (N, C, D, H, W)\n        # JAX input: (N, D, H, W, C)\n        # We will generate data in JAX format.\n        return nn.avg_pool(x,\n                           window_shape=(kernel_size, kernel_size, kernel_size),\n                           strides=(stride, stride, stride),\n                           padding=[(padding, padding), (padding, padding), (padding, padding)])\n\nbatch_size = 16\nchannels = 32\ndepth = 128\nheight = 128\nwidth = 256\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, depth, height, width, channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)\n", "description": "This code implements 46 Average Pooling 3D.\n\nGenerates a random input tensor.", "difficulty": 3}
{"id": "968b9f07-e3c7-4858-8c5e-490f0f4e5016", "code": "import json\nimport uuid\nimport os\n\ndef create_jsonl_entry(filename, content):\n    description = \"This code implements {}.\".format(filename.replace('_', ' ').replace('.py', ''))\n    docstring = \"\"\n    try:\n        # A simple way to extract a docstring from the first function if it exists\n        # This is not a robust parser.\n        start = content.find('\"\"\"')\n        if start != -1:\n            end = content.find('\"\"\"', start + 3)\n            if end != -1:\n                docstring = content[start+3:end].strip()\n                description += \"\\n\\n\" + docstring\n    except Exception as e:\n        pass\n\n    return {\n        \"id\": str(uuid.uuid4()),\n        \"code\": content,\n        \"description\": description,\n        \"difficulty\": 3, # Default difficulty\n    }\n\noutput_dir = \"level1_jax_jsonl\"\noutput_file = os.path.join(output_dir, \"level1_jax.jsonl\")\n\n# List all python files\nfiles = [f for f in os.listdir('.') if f.endswith('.py')]\n\nwith open(output_file, 'w') as f:\n    for filename in files:\n        # a file may not be a python file\n        if not filename.endswith('.py'):\n            continue\n        with open(filename, 'r') as py_file:\n            content = py_file.read()\n            entry = create_jsonl_entry(filename, content)\n            f.write(json.dumps(entry) + '\\n')\n\nprint(\"Processed {} files and created {}\".format(len(files), output_file))", "description": "This code implements process files.\n\n')\n        if start != -1:\n            end = content.find('", "difficulty": 3}
{"id": "0aa10606-17d3-4bce-bf7e-d3fa9df2cf7b", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nin_channels = 32\nout_channels = 64\nkernel_size = (5, 9)\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        # PyTorch input: (N, C, H, W)\n        # JAX input: (N, H, W, C)\n        # We will generate data in JAX format.\n        return nn.Conv(features=out_channels, kernel_size=kernel_size, use_bias=False)(x)\n\n# Test code\nbatch_size = 8\nwidth = 512\nheight = 512\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, height, width, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)\n", "description": "This code implements 62 conv standard 2D  square input  asymmetric kernel.\n\nGenerates a random input tensor.", "difficulty": 3}
{"id": "5693bd0b-9732-4222-8f0f-721b85713660", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(x):\n    \"\"\"\n    Simple model that performs a HardSigmoid activation.\n    \"\"\"\n    return nn.hard_sigmoid(x)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the input tensor [x].\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []\n", "description": "This code implements 28 HardSigmoid.\n\nSimple model that performs a HardSigmoid activation.", "difficulty": 3}
{"id": "f8626368-0bab-45be-a9c5-c1b6dd652606", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(x):\n    \"\"\"\n    Simple model that performs L2 normalization.\n    \"\"\"\n    return x / jnp.linalg.norm(x, ord=2, axis=1, keepdims=True)\n\nbatch_size = 32768\n# choose dim so total <2^31\ndim = 65535\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n    \"\"\"\n    return []\n", "description": "This code implements 39 L2Norm .\n\nSimple model that performs L2 normalization.", "difficulty": 3}
{"id": "12e2f9ab-af66-4668-93c1-82ed1f659508", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nin_channels = 32\nout_channels = 64\nkernel_size = (3, 5)\nstride = (2, 3)\npadding = (1, 2)\ndilation = (2, 1)\ngroups = 4\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        # PyTorch input: (N, C, H, W)\n        # JAX input: (N, H, W, C)\n        # We will generate data in JAX format.\n        return nn.ConvTranspose(features=out_channels,\n                                kernel_size=kernel_size,\n                                strides=stride,\n                                padding=[(padding[0], padding[0]), (padding[1], padding[1])],\n                                kernel_dilation=dilation,\n                                feature_group_count=groups,\n                                use_bias=False)(x)\n\n# Test code\nbatch_size = 16\nheight = 128\nwidth = 256\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, height, width, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)\n", "description": "This code implements 75 conv transposed 2D asymmetric input asymmetric kernel strided  grouped    padded    dilated  .\n\nGenerates a random input tensor.", "difficulty": 3}
{"id": "a1659372-385e-4687-b5bc-d0cecf22429b", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(A, B):\n    \"\"\"\n    Simple model that performs matrix multiplication (C = A * B) for upper triangular matrices.\n    \"\"\"\n    return jnp.triu(jnp.matmul(A, B))\n\nN = 4096\n\ndef get_inputs(key):\n    \"\"\"\n    Generates upper triangular matrices for testing.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing two upper triangular matrices of shape (N, N).\n    \"\"\"\n    key_a, key_b = jax.random.split(key)\n    A = jnp.triu(jax.random.normal(key_a, (N, N)))\n    B = jnp.triu(jax.random.normal(key_b, (N, N)))\n    return [A, B]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No specific initialization inputs are needed for this model.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []\n", "description": "This code implements 14 Matmul for upper triangular matrices.\n\nSimple model that performs matrix multiplication (C = A * B) for upper triangular matrices.", "difficulty": 3}
{"id": "1b66a378-6489-4dde-8303-79cd151e7c74", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(x):\n    \"\"\"\n    Simple model that performs a HardTanh activation.\n    \"\"\"\n    return nn.hard_tanh(x)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the input tensor [x].\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []\n", "description": "This code implements 32 HardTanh.\n\nSimple model that performs a HardTanh activation.", "difficulty": 3}
{"id": "1994f5b1-b723-4a74-8644-b6be01f9863d", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nreduce_dim = 1\n\ndef model(x):\n    \"\"\"\n    Simple model that performs mean reduction over a specific dimension.\n    \"\"\"\n    return jnp.mean(x, axis=reduce_dim)\n\nbatch_size = 128\ndim1 = 4096\ndim2 = 4095\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim1, dim2))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n    \"\"\"\n    return []\n", "description": "This code implements 48 Mean reduction over a dimension.\n\nSimple model that performs mean reduction over a specific dimension.", "difficulty": 3}
{"id": "9107092b-636f-4c1a-a965-a12da3e1e66a", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs Layer Normalization.\n    \"\"\"\n    @nn.compact\n    def __call__(self, x):\n        \"\"\"\n        Applies Layer Normalization to the input tensor.\n\n        Args:\n            x (jax.numpy.ndarray): Input tensor of shape (batch_size, features, dim1, dim2).\n\n        Returns:\n            jax.numpy.ndarray: Output tensor with Layer Normalization applied, same shape as input.\n        \"\"\"\n        original_shape = x.shape\n        reshaped_x = x.reshape(original_shape[0], -1)\n        normalized_x = nn.LayerNorm()(reshaped_x)\n        return normalized_x.reshape(original_shape)\n\nbatch_size = 16\nfeatures = 64\ndim1 = 256\ndim2 = 256\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, features, dim1, dim2))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)\n", "description": "This code implements 40 LayerNorm.\n\nSimple model that performs Layer Normalization.", "difficulty": 3}
{"id": "7e12a6aa-ccd1-4876-aff9-37c51bb37e72", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs Batch Normalization.\n    \"\"\"\n    @nn.compact\n    def __call__(self, x):\n        \"\"\"\n        Applies Batch Normalization to the input tensor.\n\n        Args:\n            x (jax.numpy.ndarray): Input tensor of shape (batch_size, dim1, dim2, features).\n\n        Returns:\n            jax.numpy.ndarray: Output tensor with Batch Normalization applied, same shape as input.\n        \"\"\"\n        return nn.BatchNorm(use_running_average=True)(x)\n\nbatch_size = 64\nfeatures = 64\ndim1 = 512\ndim2 = 512\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim1, dim2, features))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)\n", "description": "This code implements 33 BatchNorm.\n\nSimple model that performs Batch Normalization.", "difficulty": 3}
{"id": "867ce7f5-8948-4b06-8612-2a40bd6ee906", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nin_channels = 16\nout_channels = 128\nkernel_size = 3\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        # PyTorch input: (N, C, H, W)\n        # JAX input: (N, H, W, C)\n        # We will generate data in JAX format.\n        return nn.Conv(features=out_channels, kernel_size=(kernel_size, kernel_size), use_bias=False)(x)\n\n# Test code\nbatch_size = 16\nwidth = 1024\nheight = 1024\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, height, width, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)\n", "description": "This code implements 63 conv standard 2D  square input  square kernel.\n\nGenerates a random input tensor.", "difficulty": 3}
{"id": "7c0eafc2-f2bf-4ef7-95bf-aea26ba7e4b3", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nin_channels = 3\nout_channels = 64\nkernel_size = (3, 5, 7)\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        # PyTorch input: (N, C, W, H, D)\n        # JAX input: (N, W, H, D, C)\n        # We will generate data in JAX format.\n        return nn.Conv(features=out_channels, kernel_size=kernel_size, use_bias=False)(x)\n\n# Test code\nbatch_size = 16\nwidth = 64\nheight = 64\ndepth = 64\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, width, height, depth, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)\n", "description": "This code implements 60 conv standard 3D  square input  asymmetric kernel.\n\nGenerates a random input tensor.", "difficulty": 3}
{"id": "112cc768-6951-4141-a93e-8da2385833db", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(A, B):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B)\n    \"\"\"\n    return jnp.matmul(A, B.T)\n\nM = 1024 * 2\nK = 4096 * 2\nN = 2048 * 2\n\ndef get_inputs(key):\n    \"\"\"\n    Generates random input matrices A and B.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the two input matrices [A, B].\n    \"\"\"\n    key_a, key_b = jax.random.split(key)\n    A = jax.random.normal(key_a, (M, K))\n    B = jax.random.normal(key_b, (N, K))\n    return [A, B]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []\n", "description": "This code implements 17 Matmul with transposed B.\n\nSimple model that performs a single matrix multiplication (C = A * B)", "difficulty": 3}
{"id": "8cf2577e-bca9-4ee9-aa22-52e1b3a7c0df", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(A, B):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B) with a large K dimension\n    \"\"\"\n    return jnp.matmul(A, B)\n\nM = 256\nN = 256\nK = 131072 * 4\n\ndef get_inputs(key):\n    \"\"\"\n    Generates random input matrices A and B.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the two input matrices [A, B].\n    \"\"\"\n    key_a, key_b = jax.random.split(key)\n    A = jax.random.normal(key_a, (M, K))\n    B = jax.random.normal(key_b, (K, N))\n    return [A, B]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []\n", "description": "This code implements 6 Matmul with large K dimension .\n\nSimple model that performs a single matrix multiplication (C = A * B) with a large K dimension", "difficulty": 3}
{"id": "ffb3992e-e907-44e0-8fc5-431e457a8bbc", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(A, B):\n    \"\"\"\n    Simple model that performs a single square matrix multiplication (C = A * B)\n    \"\"\"\n    return jnp.matmul(A, B)\n\nN = 2048 * 2\n\ndef get_inputs(key):\n    \"\"\"\n    Generates random input matrices A and B.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the two input matrices [A, B].\n    \"\"\"\n    key_a, key_b = jax.random.split(key)\n    A = jax.random.normal(key_a, (N, N))\n    B = jax.random.normal(key_b, (N, N))\n    return [A, B]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []\n", "description": "This code implements 1 Square matrix multiplication .\n\nSimple model that performs a single square matrix multiplication (C = A * B)", "difficulty": 3}
{"id": "1ab74bb3-7891-47b2-88a8-9e3b1b1284a7", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(A, B):\n    \"\"\"\n    Performs 3D tensor-matrix multiplication.\n    \"\"\"\n    return jnp.matmul(A, B)\n\nN = 16\nM = 1024\nK = 2048\nL = 768\n\ndef get_inputs(key):\n    \"\"\"\n    Generates random input 3D tensor and matrix.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the input tensor and matrix [A, B].\n    \"\"\"\n    key_a, key_b = jax.random.split(key)\n    A = jax.random.normal(key_a, (N, M, K))\n    B = jax.random.normal(key_b, (K, L))\n    return [A, B]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []\n", "description": "This code implements 10 3D tensor matrix multiplication.\n\nPerforms 3D tensor-matrix multiplication.", "difficulty": 3}
{"id": "bfd3b87a-94af-446a-a7d6-f1d8d5a147bd", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(A, B):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B) where one of the matrices is tall and skinny (M >> N or N >> M)\n    \"\"\"\n    return jnp.matmul(A, B)\n\nM = 16384 * 2\nN = 16 * 2\n\ndef get_inputs(key):\n    \"\"\"\n    Generates random input matrices A and B.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the two input matrices [A, B].\n    \"\"\"\n    key_a, key_b = jax.random.split(key)\n    A = jax.random.normal(key_a, (M, N))\n    B = jax.random.normal(key_b, (N, M))\n    return [A, B]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []\n", "description": "This code implements 9 Tall skinny matrix multiplication .\n\nSimple model that performs a single matrix multiplication (C = A * B) where one of the matrices is tall and skinny (M >> N or N >> M)", "difficulty": 3}
{"id": "f7995a20-d227-4228-accc-2d77dc288b7a", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(A, B):\n    \"\"\"\n    Simple model that performs a matrix multiplication of a diagonal matrix with another matrix.\n    C = diag(A) * B\n    \"\"\"\n    return jnp.diag(A) @ B\n\nM = 4096\nN = 4096\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random 1D tensor and a 2D tensor.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the two input tensors [A, B].\n    \"\"\"\n    key_a, key_b = jax.random.split(key)\n    A = jax.random.normal(key_a, (N,))\n    B = jax.random.normal(key_b, (N, M))\n    return [A, B]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []\n", "description": "This code implements 12 Matmul with diagonal matrices .\n\nSimple model that performs a matrix multiplication of a diagonal matrix with another matrix.\n    C = diag(A) * B", "difficulty": 3}
{"id": "6c52b4bf-c5bb-463e-b28a-a23f2084ddeb", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(A, B):\n    \"\"\"\n    Performs batched matrix multiplication (C = A * B) where A, B, and C have the same batch dimension.\n    \"\"\"\n    return jnp.einsum('bij,bjk->bik', A, B)\n\nbatch_size = 128\nm = 128 * 4\nk = 256 * 4\nn = 512 * 4\n\ndef get_inputs(key):\n    \"\"\"\n    Generates random input matrices A and B.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the two input matrices [A, B].\n    \"\"\"\n    key_a, key_b = jax.random.split(key)\n    A = jax.random.normal(key_a, (batch_size, m, k))\n    B = jax.random.normal(key_b, (batch_size, k, n))\n    return [A, B]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []\n", "description": "This code implements 3 Batched matrix multiplication.\n\nPerforms batched matrix multiplication (C = A * B) where A, B, and C have the same batch dimension.", "difficulty": 3}
{"id": "3a7ddaec-ebc1-4ec4-9c43-79a522721f42", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nin_channels = 32\nout_channels = 64\nkernel_size = 3\nstride = 2\npadding = 1\ndilation = 2\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        # PyTorch input: (N, C, L)\n        # JAX input: (N, L, C)\n        # We will generate data in JAX format.\n        return nn.ConvTranspose(features=out_channels,\n                                kernel_size=(kernel_size,),\n                                strides=(stride,),\n                                padding=[(padding, padding)],\n                                kernel_dilation=(dilation,),\n                                use_bias=False)(x)\n\n# Test code\nbatch_size = 16\nlength = 131072\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, length, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)\n", "description": "This code implements 79 conv transposed 1D asymmetric input square kernel   padded    strided    dilated  .\n\nGenerates a random input tensor.", "difficulty": 3}
{"id": "b8aa0cb9-28f0-447b-a549-221f01af3ac2", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(x):\n    \"\"\"\n    Simple model that performs a GELU activation.\n    \"\"\"\n    return nn.gelu(x)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the input tensor [x].\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []\n", "description": "This code implements 26 GELU .\n\nSimple model that performs a GELU activation.", "difficulty": 3}
{"id": "00f6f3cc-b971-402d-847a-6b9cde242fe4", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nin_channels = 32\nout_channels = 32\nkernel_size = 3\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        # PyTorch input: (N, C, H, W)\n        # JAX input: (N, H, W, C)\n        # We will generate data in JAX format.\n        return nn.ConvTranspose(features=out_channels, kernel_size=(kernel_size, kernel_size), use_bias=False)(x)\n\n# Test code\nbatch_size = 8\nheight_in = 512\nwidth_in = 1024\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, height_in, width_in, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)\n", "description": "This code implements 71 conv transposed 2D  asymmetric input  square kernel.\n\nGenerates a random input tensor.", "difficulty": 3}
{"id": "e311018e-dc79-496e-bcc1-4afd3625cde1", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nreduce_dim = 1\n\ndef model(x):\n    \"\"\"\n    Simple model that performs Argmax over a specified dimension.\n    \"\"\"\n    return jnp.argmax(x, axis=reduce_dim)\n\nbatch_size = 128\ndim1 = 4096\ndim2 = 4095\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim1, dim2))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n    \"\"\"\n    return []\n", "description": "This code implements 51 Argmax over a dimension.\n\nSimple model that performs Argmax over a specified dimension.", "difficulty": 3}
{"id": "23ccfdf1-9e1c-48c1-bef9-8eba489dc53f", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(x):\n    \"\"\"\n    Simple model that performs a Softsign activation.\n    \"\"\"\n    return nn.soft_sign(x)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the input tensor [x].\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []\n", "description": "This code implements 30 Softsign.\n\nSimple model that performs a Softsign activation.", "difficulty": 3}
{"id": "6adba5c1-682e-4038-a994-bd7a8e06773e", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(x):\n    \"\"\"\n    Simple model that performs a Tanh activation.\n    \"\"\"\n    return nn.tanh(x)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the input tensor [x].\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []\n", "description": "This code implements 22 Tanh.\n\nSimple model that performs a Tanh activation.", "difficulty": 3}
{"id": "999d6a85-b29e-441b-bf5a-b9df71502a7e", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(x):\n    \"\"\"\n    Simple model that performs an ELU activation.\n    \"\"\"\n    return nn.elu(x, alpha=1.0)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the input tensor [x].\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []\n", "description": "This code implements 31 ELU.\n\nSimple model that performs an ELU activation.", "difficulty": 3}
{"id": "9c1407ed-fd38-4473-bae5-e1bc09cac196", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(x):\n    \"\"\"\n    Simple model that performs a SELU activation.\n    \"\"\"\n    return nn.selu(x)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the input tensor [x].\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []\n", "description": "This code implements 27 SELU .\n\nSimple model that performs a SELU activation.", "difficulty": 3}
{"id": "92580ee7-e09a-4922-b1bb-8c63f64f888e", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nin_channels = 64\nout_channels = 64\nkernel_size = 3\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        # PyTorch input: (N, C, H, W)\n        # JAX input: (N, H, W, C)\n        # We will generate data in JAX format.\n        return nn.ConvTranspose(features=out_channels, kernel_size=(kernel_size, kernel_size), use_bias=False)(x)\n\n# Test code\nbatch_size = 8\nheight = 1024\nwidth = 1024\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, height, width, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)\n", "description": "This code implements 57 conv transposed 2D  square input  square kernel.\n\nGenerates a random input tensor.", "difficulty": 3}
{"id": "142b5c18-5369-4ce2-90cb-19889bc506aa", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(x):\n    \"\"\"\n    Simple model that performs a Swish activation.\n    \"\"\"\n    return nn.swish(x)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the input tensor [x].\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []\n", "description": "This code implements 25 Swish.\n\nSimple model that performs a Swish activation.", "difficulty": 3}
{"id": "4b9e94bc-7c68-4608-8cdf-2e6eb769db6f", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nin_channels = 32\nout_channels = 16\nkernel_size = (3, 5, 7)\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        # PyTorch input: (N, C, D, H, W)\n        # JAX input: (N, D, H, W, C)\n        # We will generate data in JAX format.\n        return nn.ConvTranspose(features=out_channels, kernel_size=kernel_size, use_bias=False)(x)\n\n# Test code\nbatch_size = 16\ndepth_in = 16\nheight_in = 32\nwidth_in = 64\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, depth_in, height_in, width_in, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)\n", "description": "This code implements 58 conv transposed 3D  asymmetric input  asymmetric kernel.\n\nGenerates a random input tensor.", "difficulty": 3}
{"id": "ca79a6c2-5e8d-411d-b867-a48e952149a3", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nin_channels = 32\nout_channels = 64\nkernel_size = 3\nstride = 2\npadding = 1\ndilation = 2\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        # PyTorch input: (N, C, D, H, W)\n        # JAX input: (N, D, H, W, C)\n        # We will generate data in JAX format.\n        return nn.ConvTranspose(features=out_channels,\n                                kernel_size=(kernel_size, kernel_size, kernel_size),\n                                strides=(stride, stride, stride),\n                                padding=[(padding, padding), (padding, padding), (padding, padding)],\n                                kernel_dilation=(dilation, dilation, dilation),\n                                use_bias=False)(x)\n\n# Test code\nbatch_size = 16\ndepth = 16\nheight = 32\nwidth = 32\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, depth, height, width, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)\n", "description": "This code implements 77 conv transposed 3D square input square kernel   padded    dilated    strided  .\n\nGenerates a random input tensor.", "difficulty": 3}
{"id": "21ad986b-68ff-462a-ab6c-6a35aa8c0652", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nin_channels = 3\nout_channels = 64\nkernel_size = (3, 5, 7)\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        # PyTorch input: (N, C, D, H, W)\n        # JAX input: (N, D, H, W, C)\n        # We will generate data in JAX format.\n        return nn.Conv(features=out_channels, kernel_size=kernel_size, use_bias=False)(x)\n\n# Test code\nbatch_size = 8\ndepth = 16\nheight = 128\nwidth = 128\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, depth, height, width, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)\n", "description": "This code implements 66 conv standard 3D  asymmetric input  asymmetric kernel.\n\nGenerates a random input tensor.", "difficulty": 3}
{"id": "3cadd613-1236-47f9-a3d8-8f434e0398d5", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nin_channels = 128\nout_channels = 128\nkernel_size = 3\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        # PyTorch input: (N, C, L)\n        # JAX input: (N, L, C)\n        # We will generate data in JAX format.\n        return nn.ConvTranspose(features=out_channels, kernel_size=(kernel_size,), use_bias=False)(x)\n\n# Test code\nbatch_size = 64\nlength = 65536\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, length, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)\n", "description": "This code implements 64 conv transposed 1D.\n\nGenerates a random input tensor.", "difficulty": 3}
{"id": "8acfd3d3-be83-4a9c-bd2d-eab4de8c9969", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nin_channels = 64\nout_channels = 128\nkernel_size = 3\nstride = 3\ndilation = 4\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        # PyTorch input: (N, C, L)\n        # JAX input: (N, L, C)\n        # We will generate data in JAX format.\n        return nn.Conv(features=out_channels,\n                       kernel_size=(kernel_size,),\n                       strides=(stride,),\n                       kernel_dilation=(dilation,),\n                       use_bias=False)(x)\n\n# Test code\nbatch_size = 64\nlength = 524280\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, length, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)\n", "description": "This code implements 76 conv standard 1D dilated strided  .\n\nGenerates a random input tensor.", "difficulty": 3}
{"id": "3b1c2ed4-7b15-447e-8c46-c3f2e5bbb536", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(x):\n    \"\"\"\n    Simple model that performs a Softplus activation.\n    \"\"\"\n    return nn.softplus(x)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the input tensor [x].\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []\n", "description": "This code implements 29 Softplus.\n\nSimple model that performs a Softplus activation.", "difficulty": 3}
{"id": "aacb121b-d739-4fe0-815b-2fb68cb306be", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nin_channels = 32\nout_channels = 32\nkernel_size = (3, 5, 7)\nstride = (2, 2, 2)\npadding = (1, 2, 3)\noutput_padding = (1, 1, 1)\ngroups = 4\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        # PyTorch input: (N, C, D, H, W)\n        # JAX input: (N, D, H, W, C)\n        # We will generate data in JAX format.\n        \n        # Adjust padding to account for output_padding\n        padding_jax = [\n            (padding[0], padding[0] - output_padding[0]),\n            (padding[1], padding[1] - output_padding[1]),\n            (padding[2], padding[2] - output_padding[2])\n        ]\n        \n        return nn.ConvTranspose(features=out_channels,\n                                kernel_size=kernel_size,\n                                strides=stride,\n                                padding=padding_jax,\n                                feature_group_count=groups,\n                                use_bias=False)(x)\n\n# Test code\nbatch_size = 8\ndepth = 12\nheight = 24\nwidth = 48\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, depth, height, width, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)\n", "description": "This code implements 72 conv transposed 3D asymmetric input asymmetric kernel   strided padded grouped .\n\nGenerates a random input tensor.", "difficulty": 3}
{"id": "070f54e3-19d8-46a1-a5b1-a5e8b983ae79", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nin_channels = 32\nout_channels = 64\nkernel_size = 5\nstride = 1\npadding = 0\ndilation = 3\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        # PyTorch input: (N, C, L)\n        # JAX input: (N, L, C)\n        # We will generate data in JAX format.\n        return nn.ConvTranspose(features=out_channels,\n                                kernel_size=(kernel_size,),\n                                strides=(stride,),\n                                padding=[(padding, padding)],\n                                kernel_dilation=(dilation,),\n                                use_bias=False)(x)\n\n# Test code\nbatch_size = 32\nlength = 131072\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, length, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)\n", "description": "This code implements 74 conv transposed 1D dilated.\n\nGenerates a random input tensor.", "difficulty": 3}
{"id": "8eb0cdca-f49c-4eb6-919e-b05a5ac445e9", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nin_channels = 64\nout_channels = 128\nkernel_size = 3\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        # PyTorch input: (N, C, H, W)\n        # JAX input: (N, H, W, C)\n        # We will generate data in JAX format.\n        return nn.Conv(features=out_channels, kernel_size=(kernel_size, kernel_size), use_bias=False)(x)\n\n# Test code\nbatch_size = 8\n# smaller spatial dims\nheight = 512\nwidth = 1024\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, height, width, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)\n", "description": "This code implements 55 conv standard 2D  asymmetric input  square kernel.\n\nGenerates a random input tensor.", "difficulty": 3}
{"id": "407b1a6c-6e0f-4a3e-963f-a89aca4d38a7", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        # PyTorch input: (N, C, H, W)\n        # JAX input: (N, H, W, C)\n        # We will generate data in JAX format.\n        x = nn.Conv(features=96, kernel_size=(11, 11), strides=(4, 4), padding=[(2,2),(2,2)])(x)\n        return x\n\n# Test code\nbatch_size = 256\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, 224, 224, 3))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)\n", "description": "This code implements 50 conv standard 2D  square input  square kernel.\n\nGenerates a random input tensor.", "difficulty": 3}
{"id": "c1db9524-d3db-459b-a738-377c1bdea89b", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(A, B):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B) with irregular shapes\n    \"\"\"\n    return jnp.matmul(A, B)\n\nM = 8205\nK = 2949\nN = 5921\n\ndef get_inputs(key):\n    \"\"\"\n    Generates random input matrices A and B.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the two input matrices [A, B].\n    \"\"\"\n    key_a, key_b = jax.random.split(key)\n    A = jax.random.normal(key_a, (M, K))\n    B = jax.random.normal(key_b, (K, N))\n    return [A, B]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []\n", "description": "This code implements 8 Matmul with irregular shapes .\n\nSimple model that performs a single matrix multiplication (C = A * B) with irregular shapes", "difficulty": 3}
{"id": "d5f10f40-0b09-48a5-8f39-f27ff77eb871", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(A, B):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B)\n    \"\"\"\n    return jnp.matmul(A, B)\n\nM = 1024 * 2\nK = 4096 * 2\nN = 2048 * 2\n\ndef get_inputs(key):\n    \"\"\"\n    Generates random input matrices A and B.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the two input matrices [A, B].\n    \"\"\"\n    key_a, key_b = jax.random.split(key)\n    A = jax.random.normal(key_a, (M, K))\n    B = jax.random.normal(key_b, (K, N))\n    return [A, B]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []\n", "description": "This code implements 2 Standard matrix multiplication .\n\nSimple model that performs a single matrix multiplication (C = A * B)", "difficulty": 3}
{"id": "e4984384-0573-4e6e-804e-63829111b03b", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nin_channels = 64\nout_channels = 128\nkernel_size = 3\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        # PyTorch input: (N, C, L)\n        # JAX input: (N, L, C)\n        # We will generate data in JAX format.\n        return nn.Conv(features=out_channels, kernel_size=(kernel_size,), use_bias=False)(x)\n\n# Test code\nbatch_size = 32\nlength = 131072\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, length, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)\n", "description": "This code implements 67 conv standard 1D.\n\nGenerates a random input tensor.", "difficulty": 3}
{"id": "9ce78824-9003-4485-8160-43126a2266a1", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(A, B):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B) with A and B being symmetric matrices.\n    \"\"\"\n    return jnp.matmul(A, B)\n\nN = 4096\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a pair of random symmetric matrices for testing.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: List containing two symmetric tensors A and B.\n    \"\"\"\n    key_a, key_b = jax.random.split(key)\n    A = jax.random.normal(key_a, (N, N))\n    A = (A + A.T) / 2  # Ensure symmetry\n    B = jax.random.normal(key_b, (N, N))\n    B = (B + B.T) / 2  # Ensure symmetry\n    return [A, B]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No specific initialization inputs needed for this model.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: Empty list.\n    \"\"\"\n    return []\n", "description": "This code implements 13 Matmul for symmetric matrices.\n\nSimple model that performs a single matrix multiplication (C = A * B) with A and B being symmetric matrices.", "difficulty": 3}
{"id": "d3d5af30-7814-41fc-8989-7d37f5f683c6", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(A, B):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B)\n    \"\"\"\n    return jnp.matmul(A.T, B.T)\n\nM = 1024 * 2\nK = 4096 * 2\nN = 2048 * 2\n\ndef get_inputs(key):\n    \"\"\"\n    Generates random input matrices A and B.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the two input matrices [A, B].\n    \"\"\"\n    key_a, key_b = jax.random.split(key)\n    A = jax.random.normal(key_a, (K, M))\n    B = jax.random.normal(key_b, (N, K))\n    return [A, B]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []\n", "description": "This code implements 18 Matmul with transposed both.\n\nSimple model that performs a single matrix multiplication (C = A * B)", "difficulty": 3}
{"id": "b7f6aa88-e2fa-4cac-8991-49089932c56c", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nin_channels = 3\nout_channels = 64\nkernel_size = 3\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        # PyTorch input: (N, C, H, W, D)\n        # JAX input: (N, H, W, D, C)\n        # We will generate data in JAX format.\n        return nn.Conv(features=out_channels, kernel_size=(kernel_size, kernel_size, 1), use_bias=False)(x)\n\n# Test code\nbatch_size = 16\nwidth = 256\nheight = 256\ndepth = 10\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, height, width, depth, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)\n", "description": "This code implements 59 conv standard 3D  asymmetric input  square kernel.\n\nGenerates a random input tensor.", "difficulty": 3}
{"id": "e38189f3-b76f-484c-b09d-5d4afbc7e674", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(x):\n    \"\"\"\n    Simple model that performs a ReLU activation.\n    \"\"\"\n    return nn.relu(x)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the input tensor [x].\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []\n", "description": "This code implements 19 ReLU.\n\nSimple model that performs a ReLU activation.", "difficulty": 3}
{"id": "2a223848-1ecf-4c03-bf3f-b2090c2b199f", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(x):\n    \"\"\"\n    Simple model that performs a Sigmoid activation.\n    \"\"\"\n    return nn.sigmoid(x)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the input tensor [x].\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []\n", "description": "This code implements 21 Sigmoid.\n\nSimple model that performs a Sigmoid activation.", "difficulty": 3}
{"id": "4f767f11-52c0-4385-9798-e168cb25683c", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nkernel_size = 11\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        # PyTorch input: (N, C, H, W)\n        # JAX input: (N, H, W, C)\n        # We will generate data in JAX format.\n        return nn.avg_pool(x,\n                           window_shape=(kernel_size, kernel_size),\n                           strides=(kernel_size, kernel_size),\n                           padding='VALID')\n\nbatch_size = 16\nchannels = 64\nheight = 2048\nwidth = 2048\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, height, width, channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)\n", "description": "This code implements 45 Average Pooling 2D.\n\nGenerates a random input tensor.", "difficulty": 3}
{"id": "dc1e5c8b-d9d6-4a2b-8aee-2f5c32fde8b6", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nin_channels = 48\nout_channels = 48\nkernel_size = 3\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        # PyTorch input: (N, C, D, H, W)\n        # JAX input: (N, D, H, W, C)\n        # We will generate data in JAX format.\n        return nn.ConvTranspose(features=out_channels, kernel_size=(kernel_size, kernel_size, kernel_size), use_bias=False)(x)\n\n# Test code\nbatch_size = 8\ndepth = 64\nheight = 64\nwidth = 64\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, depth, height, width, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)\n", "description": "This code implements 61 conv transposed 3D  square input  square kernel.\n\nGenerates a random input tensor.", "difficulty": 3}
{"id": "192882fc-99f5-4fd9-994d-29880762560f", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(x):\n    \"\"\"\n    Simple model that performs a LogSoftmax activation.\n    \"\"\"\n    return nn.log_softmax(x, axis=1)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the input tensor [x].\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []\n", "description": "This code implements 24 LogSoftmax.\n\nSimple model that performs a LogSoftmax activation.", "difficulty": 3}
{"id": "ec9e3aff-cfa7-41dd-a4a7-72d65abc8d1d", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(x):\n    \"\"\"\n    Simple model that performs a LeakyReLU activation.\n    \"\"\"\n    return nn.leaky_relu(x, negative_slope=0.01)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the input tensor [x].\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []\n", "description": "This code implements 20 LeakyReLU.\n\nSimple model that performs a LeakyReLU activation.", "difficulty": 3}
{"id": "1b5d8c99-1744-4d39-b3a3-abebdfbf5cb0", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(A, s):\n    \"\"\"\n    Simple model that performs a matrix-scalar multiplication (C = A * s)\n    \"\"\"\n    return A * s\n\nM = 16384 * 4\nN = 4096 * 4\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input matrix and a scalar.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the input matrix and scalar [A, s].\n    \"\"\"\n    A = jax.random.normal(key, (M, N))\n    s = 3.14\n    return [A, s]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []\n", "description": "This code implements 5 Matrix scalar multiplication.\n\nSimple model that performs a matrix-scalar multiplication (C = A * s)", "difficulty": 3}
{"id": "90ce8d43-5516-48eb-999e-bfd36cf113f6", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(A, B):\n    \"\"\"\n    Simple model that performs a matrix multiplication (C = A * B) where A and B are lower triangular matrices. \n    \"\"\"\n    return jnp.tril(jnp.matmul(A, B))\n\nM = 4096\n\ndef get_inputs(key):\n    \"\"\"\n    Generates lower triangular matrices for testing.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing two lower triangular matrices of shape (M, M).\n    \"\"\"\n    key_a, key_b = jax.random.split(key)\n    A = jax.random.normal(key_a, (M, M))\n    B = jax.random.normal(key_b, (M, M))\n    A = jnp.tril(A)\n    B = jnp.tril(B)\n    return [A, B]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []\n", "description": "This code implements 15 Matmul for lower triangular matrices.\n\nSimple model that performs a matrix multiplication (C = A * B) where A and B are lower triangular matrices.", "difficulty": 3}
