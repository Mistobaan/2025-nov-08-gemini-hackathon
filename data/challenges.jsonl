{"id":"6c6f5c6f-99a5-4c3a-a028-89c5e1424960","title":"Stable Softmax With JAX","difficulty":2,"description":"### Task\nImplement a numerically stable softmax function using only JAX primitives.\n\n### Requirements\n- Create a function named `stable_softmax` that accepts a `jnp.ndarray`.\n- Use `jax.numpy` operations so the function can be `jit` compiled and `vmap`-ed.\n- Subtract the row-wise max before exponentiating and keep the result normalized.\n\n### Validation Snippet\n```python\nx = jnp.array([[1.0, 2.0, 3.0], [1000.0, 1001.0, 1002.0]])\nresult = stable_softmax(x)\nassert jnp.allclose(result.sum(axis=-1), jnp.ones(x.shape[0]))\n```\n","code":"import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef stable_softmax(x: jnp.ndarray) -> jnp.ndarray:\n    \"\"\"Return the numerically stable softmax of x.\"\"\"\n    raise NotImplementedError(\"Implement me using JAX primitives only\")\n"}
{"id":"0c5feaba-c818-4ac1-8bea-e89c14a2f0cf","title":"JIT-Tuned Training Step","difficulty":3,"description":"### Task\nWrite a single JIT-compiled training step for logistic regression in JAX.\n\n### Requirements\n- Assume you receive parameters `(w, b)` and a mini-batch `(features, labels)`.\n- Compute predictions with `jax.nn.sigmoid`, the binary cross entropy loss, and gradients via `jax.grad`.\n- Return the updated parameters and the scalar loss so the caller can log it.\n\n### Notes\nFocus on clarity and make sure the shapes work for both dense and batched inputs.\n","code":"import jax\nimport jax.numpy as jnp\nfrom typing import Tuple\n\nArray = jax.Array\n\ndef sigmoid_cross_entropy(logits: Array, labels: Array) -> Array:\n    return -labels * jax.nn.log_sigmoid(logits) - (1.0 - labels) * jax.nn.log_sigmoid(-logits)\n\n@jax.jit\ndef train_step(params: Tuple[Array, Array], batch: Tuple[Array, Array], learning_rate: float) -> Tuple[Tuple[Array, Array], Array]:\n    \"\"\"Run one training step and return (new_params, loss).\"\"\"\n\n    raise NotImplementedError(\"Use jax.grad to differentiate the loss and update params\")\n"}
{"id":"54cfa124-f5cb-4bd6-9d51-2460ea5a12d3","title":"Custom Gradient RMSNorm","difficulty":4,"description":"### Task\nImplement RMSNorm with a handcrafted custom gradient using `jax.custom_vjp`.\n\n### Requirements\n- The forward pass should follow the standard RMSNorm definition with an optional learnable scale.\n- Register forward and backward rules via `custom_vjp` so the backward path reuses intermediate statistics without recomputing them.\n- Demonstrate that the implementation works under `jax.jit` and `jax.vmap`.\n\n### Acceptance Criteria\nProvide a short doctest-style example that exercises the forward call and checks gradients with `jax.test_util.check_grads`.\n","code":"import jax\nimport jax.numpy as jnp\n\n@jax.custom_vjp\ndef rmsnorm(x: jax.Array, scale: jax.Array | None = None, eps: float = 1e-6) -> jax.Array:\n    \"\"\"Compute RMSNorm over the last dimension.\"\"\"\n    raise NotImplementedError(\"Return the normalized tensor (and apply scale if provided)\")\n\n# TODO: define forward/backward functions with @rmsnorm.defvjp\n"}
