{"id": "5d8c7463-1d18-48e5-991f-dc35448a034a", "title": "This code implements  Matrix vector multiplication", "difficulty": "easy", "description": "### Challenge Brief\nSimple model that performs matrix-vector multiplication (C = A * B). Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\nM = 256 * 8 # 2048\nK = 131072 * 8 # 1048576\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(A, B):\n    \"\"\"\n    Simple model that performs matrix-vector multiplication (C = A * B).\n    \"\"\"\n    return jnp.matmul(A, B)\n\nM = 256 * 8 # 2048\nK = 131072 * 8 # 1048576\n\ndef get_inputs(key):\n    \"\"\"\n    Generates random input matrix and vector.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the input matrix and vector [A, B].\n    \"\"\"\n    key_a, key_b = jax.random.split(key)\n    A = jax.random.normal(key_a, (M, K))\n    B = jax.random.normal(key_b, (K, 1))\n    return [A, B]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []\n", "student_template": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(A, B):\n    \"\"\"\n    Simple model that performs matrix-vector multiplication (C = A * B).\n    \"\"\"\n    # TODO: Replace this placeholder with your implementation.\n    # Helpful hints:\n    # - Review the tensors emitted by `get_inputs` so your function signature stays compatible.\n    # - Keep every computation inside JAX/Flax; avoid NumPy on the host to stay TPU-ready.\n    # - Use `jnp.matmul` (or the `@` operator) to perform the core multiply without leaving JAX.\n    # - Reuse the provided PRNG splitting strategy so shapes stay reproducible.\n    raise NotImplementedError('Implement the Matrix vector multiplication logic')\n\nM = 256 * 8 # 2048\nK = 131072 * 8 # 1048576\n\ndef get_inputs(key):\n    \"\"\"\n    Generates random input matrix and vector.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the input matrix and vector [A, B].\n    \"\"\"\n    key_a, key_b = jax.random.split(key)\n    A = jax.random.normal(key_a, (M, K))\n    B = jax.random.normal(key_b, (K, 1))\n    return [A, B]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **Matrix vector multiplication** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "8cbe5a8b-7f62-4c68-b4b1-4f87e77e65f1", "title": "This code implements  Matmul with small K dimension", "difficulty": "easy", "description": "### Challenge Brief\nSimple model that performs a single matrix multiplication (C = A * B) with a small K dimension Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\nM = 16384 * 2\nN = 16384 * 2\nK = 32 * 2\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(A, B):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B) with a small K dimension\n    \"\"\"\n    return jnp.matmul(A, B)\n\nM = 16384 * 2\nN = 16384 * 2\nK = 32 * 2\n\ndef get_inputs(key):\n    \"\"\"\n    Generates random input matrices A and B.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the two input matrices [A, B].\n    \"\"\"\n    key_a, key_b = jax.random.split(key)\n    A = jax.random.normal(key_a, (M, K))\n    B = jax.random.normal(key_b, (K, N))\n    return [A, B]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []\n", "student_template": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(A, B):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B) with a small K dimension\n    \"\"\"\n    # TODO: Replace this placeholder with your implementation.\n    # Helpful hints:\n    # - Review the tensors emitted by `get_inputs` so your function signature stays compatible.\n    # - Keep every computation inside JAX/Flax; avoid NumPy on the host to stay TPU-ready.\n    # - Use `jnp.matmul` (or the `@` operator) to perform the core multiply without leaving JAX.\n    # - Reuse the provided PRNG splitting strategy so shapes stay reproducible.\n    raise NotImplementedError('Implement the Matmul with small K dimension logic')\n\nM = 16384 * 2\nN = 16384 * 2\nK = 32 * 2\n\ndef get_inputs(key):\n    \"\"\"\n    Generates random input matrices A and B.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the two input matrices [A, B].\n    \"\"\"\n    key_a, key_b = jax.random.split(key)\n    A = jax.random.normal(key_a, (M, K))\n    B = jax.random.normal(key_b, (K, N))\n    return [A, B]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **Matmul with small K dimension** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "727f4725-6843-4658-a87c-e0c84a3d2b1c", "title": "This code implements  Matmul with transposed A", "difficulty": "easy", "description": "### Challenge Brief\nSimple model that performs a single matrix multiplication (C = A * B) Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\nM = 1024 * 2\nK = 4096 * 2\nN = 2048 * 2\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(A, B):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B)\n    \"\"\"\n    return jnp.matmul(A.T, B)\n\nM = 1024 * 2\nK = 4096 * 2\nN = 2048 * 2\n\ndef get_inputs(key):\n    \"\"\"\n    Generates random input matrices A and B.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the two input matrices [A, B].\n    \"\"\"\n    key_a, key_b = jax.random.split(key)\n    A = jax.random.normal(key_a, (K, M))\n    B = jax.random.normal(key_b, (K, N))\n    return [A, B]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []\n", "student_template": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(A, B):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B)\n    \"\"\"\n    # TODO: Replace this placeholder with your implementation.\n    # Helpful hints:\n    # - Review the tensors emitted by `get_inputs` so your function signature stays compatible.\n    # - Keep every computation inside JAX/Flax; avoid NumPy on the host to stay TPU-ready.\n    # - Use `jnp.matmul` (or the `@` operator) to perform the core multiply without leaving JAX.\n    # - Pay attention to the explicit transpose operations (e.g., `A.T`, `B.T`) before multiplying.\n    # - Reuse the provided PRNG splitting strategy so shapes stay reproducible.\n    raise NotImplementedError('Implement the Matmul with transposed A logic')\n\nM = 1024 * 2\nK = 4096 * 2\nN = 2048 * 2\n\ndef get_inputs(key):\n    \"\"\"\n    Generates random input matrices A and B.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the two input matrices [A, B].\n    \"\"\"\n    key_a, key_b = jax.random.split(key)\n    A = jax.random.normal(key_a, (K, M))\n    B = jax.random.normal(key_b, (K, N))\n    return [A, B]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **Matmul with transposed A** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "c8aee113-ca0c-403e-9694-aa7a60da8e2b", "title": "This code implements  Max Pooling D", "difficulty": "medium", "description": "### Challenge Brief\nFinish implementing **Max Pooling D** on TPU hardware. Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\nkernel_size = 3\nstride = 2\npadding = 1\ndilation = 3\nbatch_size = 16\nchannels = 32\ndim1 = 128\ndim2 = 128\ndim3 = 128\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nkernel_size = 3\nstride = 2\npadding = 1\ndilation = 3\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        # PyTorch input: (N, C, D, H, W)\n        # JAX input: (N, D, H, W, C)\n        # We will generate data in JAX format.\n        return nn.max_pool(x,\n                           window_shape=(kernel_size, kernel_size, kernel_size),\n                           strides=(stride, stride, stride),\n                           padding=[(padding, padding), (padding, padding), (padding, padding)],\n                           window_dilation=(dilation, dilation, dilation))\n\nbatch_size = 16\nchannels = 32\ndim1 = 128\ndim2 = 128\ndim3 = 128\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim1, dim2, dim3, channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)\n", "student_template": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nkernel_size = 3\nstride = 2\npadding = 1\ndilation = 3\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        \"\"\"\n        Complete the Max Pooling D forward pass.\n        \"\"\"\n        # TODO: Replace this placeholder with your implementation.\n        # Helpful hints:\n        # - Review the tensors emitted by `get_inputs` so your function signature stays compatible.\n        # - Keep every computation inside JAX/Flax; avoid NumPy on the host to stay TPU-ready.\n        # - Mirror the provided window/stride/padding/dilation hyperparameters when calling `nn.max_pool`.\n        # - Reuse the provided PRNG splitting strategy so shapes stay reproducible.\n        raise NotImplementedError('Implement the Max Pooling D logic')\n\nbatch_size = 16\nchannels = 32\ndim1 = 128\ndim2 = 128\ndim3 = 128\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim1, dim2, dim3, channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **Max Pooling D** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "0f2f65e8-39e2-4840-8c91-dd5231f11a26", "title": "This code implements  conv transposed D  square input  asymmetric kernel", "difficulty": "hard", "description": "### Challenge Brief\nFinish implementing **conv transposed D square input asymmetric kernel** on TPU hardware. Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\nin_channels = 64\nout_channels = 64\nkernel_size = (3, 7)\nbatch_size = 8\nwidth = 512\nheight = 512\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nin_channels = 64\nout_channels = 64\nkernel_size = (3, 7)\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        # PyTorch input: (N, C, H, W)\n        # JAX input: (N, H, W, C)\n        # We will generate data in JAX format.\n        return nn.ConvTranspose(features=out_channels, kernel_size=kernel_size, use_bias=False)(x)\n\n# Test code\nbatch_size = 8\nwidth = 512\nheight = 512\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, height, width, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)\n", "student_template": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nin_channels = 64\nout_channels = 64\nkernel_size = (3, 7)\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        \"\"\"\n        Complete the conv transposed D square input asymmetric kernel forward pass.\n        \"\"\"\n        # TODO: Replace this placeholder with your implementation.\n        # Helpful hints:\n        # - Review the tensors emitted by `get_inputs` so your function signature stays compatible.\n        # - Keep every computation inside JAX/Flax; avoid NumPy on the host to stay TPU-ready.\n        # - Instantiate and call `nn.Conv` with the channel counts shown in the scaffold.\n        # - Instantiate `nn.ConvTranspose` with the provided strides, kernel size, and channel config.\n        # - Reuse the provided PRNG splitting strategy so shapes stay reproducible.\n        raise NotImplementedError('Implement the conv transposed D square input asymmetric kernel logic')\n\n# Test code\nbatch_size = 8\nwidth = 512\nheight = 512\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, height, width, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **conv transposed D square input asymmetric kernel** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "4568f2de-3667-411d-beb9-6f5777f1a19f", "title": "This code implements  conv standard D  asymmetric input  asymmetric kernel", "difficulty": "hard", "description": "### Challenge Brief\nFinish implementing **conv standard D asymmetric input asymmetric kernel** on TPU hardware. Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\nin_channels = 64\nout_channels = 128\nkernel_size = (5, 7)\nbatch_size = 8\nheight = 512\nwidth = 256\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nin_channels = 64\nout_channels = 128\nkernel_size = (5, 7)\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        # PyTorch input: (N, C, H, W)\n        # JAX input: (N, H, W, C)\n        # We will generate data in JAX format.\n        return nn.Conv(features=out_channels, kernel_size=kernel_size, use_bias=False)(x)\n\n# Test code\nbatch_size = 8\nheight = 512\nwidth = 256\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, height, width, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)\n", "student_template": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nin_channels = 64\nout_channels = 128\nkernel_size = (5, 7)\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        \"\"\"\n        Complete the conv standard D asymmetric input asymmetric kernel forward pass.\n        \"\"\"\n        # TODO: Replace this placeholder with your implementation.\n        # Helpful hints:\n        # - Review the tensors emitted by `get_inputs` so your function signature stays compatible.\n        # - Keep every computation inside JAX/Flax; avoid NumPy on the host to stay TPU-ready.\n        # - Instantiate and call `nn.Conv` with the channel counts shown in the scaffold.\n        # - Reuse the provided PRNG splitting strategy so shapes stay reproducible.\n        raise NotImplementedError('Implement the conv standard D asymmetric input asymmetric kernel logic')\n\n# Test code\nbatch_size = 8\nheight = 512\nwidth = 256\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, height, width, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **conv standard D asymmetric input asymmetric kernel** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "559c8749-b350-4308-8be7-b57659a74ca4", "title": "This code implements  GroupNorm", "difficulty": "medium", "description": "### Challenge Brief\nSimple model that performs Group Normalization. Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\nnum_groups = 8\nbatch_size = 112  # scaled up\nfeatures = 64\ndim1 = 512\ndim2 = 512\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nnum_groups = 8\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs Group Normalization.\n    \"\"\"\n    @nn.compact\n    def __call__(self, x):\n        \"\"\"\n        Applies Group Normalization to the input tensor.\n\n        Args:\n            x (jax.numpy.ndarray): Input tensor of shape (batch_size, dim1, dim2, features).\n\n        Returns:\n            jax.numpy.ndarray: Output tensor with Group Normalization applied, same shape as input.\n        \"\"\"\n        return nn.GroupNorm(num_groups=num_groups)(x)\n\nbatch_size = 112  # scaled up\nfeatures = 64\ndim1 = 512\ndim2 = 512\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim1, dim2, features))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)\n", "student_template": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nnum_groups = 8\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs Group Normalization.\n    \"\"\"\n    @nn.compact\n    def __call__(self, x):\n        \"\"\"\n        Applies Group Normalization to the input tensor.\n        \n        Args:\n            x (jax.numpy.ndarray): Input tensor of shape (batch_size, dim1, dim2, features).\n        \n        Returns:\n            jax.numpy.ndarray: Output tensor with Group Normalization applied, same shape as input.\n        \"\"\"\n        # TODO: Replace this placeholder with your implementation.\n        # Helpful hints:\n        # - Review the tensors emitted by `get_inputs` so your function signature stays compatible.\n        # - Keep every computation inside JAX/Flax; avoid NumPy on the host to stay TPU-ready.\n        # - Pass the correct `num_groups` into `nn.GroupNorm` before normalizing.\n        # - Reuse the provided PRNG splitting strategy so shapes stay reproducible.\n        raise NotImplementedError('Implement the GroupNorm logic')\n\nbatch_size = 112  # scaled up\nfeatures = 64\ndim1 = 512\ndim2 = 512\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim1, dim2, features))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **GroupNorm** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "ec4d3496-b3a1-4d8b-b8d4-939908c31341", "title": "This code implements  Softmax", "difficulty": "easy", "description": "### Challenge Brief\nSimple model that performs a Softmax activation. Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\nbatch_size = 4096\ndim = 393216\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(x):\n    \"\"\"\n    Simple model that performs a Softmax activation.\n    \"\"\"\n    return nn.softmax(x, axis=1)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the input tensor [x].\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []\n", "student_template": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(x):\n    \"\"\"\n    Simple model that performs a Softmax activation.\n    \"\"\"\n    # TODO: Replace this placeholder with your implementation.\n    # Helpful hints:\n    # - Review the tensors emitted by `get_inputs` so your function signature stays compatible.\n    # - Keep every computation inside JAX/Flax; avoid NumPy on the host to stay TPU-ready.\n    # - Apply `nn.softmax` across the class dimension for probabilities.\n    # - Reuse the provided PRNG splitting strategy so shapes stay reproducible.\n    raise NotImplementedError('Implement the Softmax logic')\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the input tensor [x].\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **Softmax** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "a419c16f-0293-4af6-88c2-2c3e24cc2eab", "title": "This code implements  Argmin over a dimension", "difficulty": "easy", "description": "### Challenge Brief\nSimple model that finds the index of the minimum value along a specified dimension. Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\nreduce_dim = 1\nbatch_size = 128\ndim1 = 4096\ndim2 = 4095\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nreduce_dim = 1\n\ndef model(x):\n    \"\"\"\n    Simple model that finds the index of the minimum value along a specified dimension.\n    \"\"\"\n    return jnp.argmin(x, axis=reduce_dim)\n\nbatch_size = 128\ndim1 = 4096\ndim2 = 4095\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim1, dim2))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n    \"\"\"\n    return []\n", "student_template": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nreduce_dim = 1\n\ndef model(x):\n    \"\"\"\n    Simple model that finds the index of the minimum value along a specified dimension.\n    \"\"\"\n    # TODO: Replace this placeholder with your implementation.\n    # Helpful hints:\n    # - Review the tensors emitted by `get_inputs` so your function signature stays compatible.\n    # - Keep every computation inside JAX/Flax; avoid NumPy on the host to stay TPU-ready.\n    # - Remember that `jnp.argmin` needs an `axis` when working on tensors.\n    # - Reuse the provided PRNG splitting strategy so shapes stay reproducible.\n    raise NotImplementedError('Implement the Argmin over a dimension logic')\n\nbatch_size = 128\ndim1 = 4096\ndim2 = 4095\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim1, dim2))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n    \"\"\"\n    return []", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **Argmin over a dimension** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "d4428536-95bc-4368-98fd-51b5b9648c80", "title": "This code implements  conv standard D  square input  square kernel", "difficulty": "hard", "description": "### Challenge Brief\nFinish implementing **conv standard D square input square kernel** on TPU hardware. Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\nin_channels = 3\nout_channels = 64\nkernel_size = 3\nbatch_size = 16\ndepth = 64\nwidth = 64\nheight = 64\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nin_channels = 3\nout_channels = 64\nkernel_size = 3\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        # PyTorch input: (N, C, D, H, W)\n        # JAX input: (N, D, H, W, C)\n        # We will generate data in JAX format.\n        return nn.Conv(features=out_channels, kernel_size=(kernel_size, kernel_size, kernel_size), use_bias=False)(x)\n\n# Test code\nbatch_size = 16\ndepth = 64\nwidth = 64\nheight = 64\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, depth, width, height, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)\n", "student_template": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nin_channels = 3\nout_channels = 64\nkernel_size = 3\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        \"\"\"\n        Complete the conv standard D square input square kernel forward pass.\n        \"\"\"\n        # TODO: Replace this placeholder with your implementation.\n        # Helpful hints:\n        # - Review the tensors emitted by `get_inputs` so your function signature stays compatible.\n        # - Keep every computation inside JAX/Flax; avoid NumPy on the host to stay TPU-ready.\n        # - Instantiate and call `nn.Conv` with the channel counts shown in the scaffold.\n        # - Reuse the provided PRNG splitting strategy so shapes stay reproducible.\n        raise NotImplementedError('Implement the conv standard D square input square kernel logic')\n\n# Test code\nbatch_size = 16\ndepth = 64\nwidth = 64\nheight = 64\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, depth, width, height, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **conv standard D square input square kernel** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "76bb7203-4319-461a-9343-754cddf39afa", "title": "This code implements  D tensor matrix multiplication", "difficulty": "easy", "description": "### Challenge Brief\nPerforms 4D tensor-matrix multiplication: C[b, i, j, k] = sum_l A[b, i, j, l] * B[l, k] Args: A (jax.numpy.ndarray): Input 4D tensor of shape (b, i, j, l) B (jax.numpy.ndarray): Input matrix of shape (l, k) Returns: jax.numpy.ndarray: Output 4D tensor of shape (b, i, j, k) Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\nb = 8\ni = 256\nj = 512\nl = 256\nk = 768\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(A, B):\n    \"\"\"\n    Performs 4D tensor-matrix multiplication: \n        C[b, i, j, k] = sum_l A[b, i, j, l] * B[l, k]\n\n    Args:\n        A (jax.numpy.ndarray): Input 4D tensor of shape (b, i, j, l)\n        B (jax.numpy.ndarray): Input matrix of shape (l, k)\n\n    Returns:\n        jax.numpy.ndarray: Output 4D tensor of shape (b, i, j, k)\n    \"\"\"\n    return jnp.einsum(\"bijl,lk->bijk\", A, B)\n\n# Test code\nb = 8\ni = 256\nj = 512\nl = 256\nk = 768\n\ndef get_inputs(key):\n    \"\"\"\n    Generates random input 4D tensor and matrix.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the input tensor and matrix [A, B].\n    \"\"\"\n    key_a, key_b = jax.random.split(key)\n    A = jax.random.normal(key_a, (b, i, j, l))\n    B = jax.random.normal(key_b, (l, k))\n    return [A, B]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []\n", "student_template": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(A, B):\n    \"\"\"\n    Performs 4D tensor-matrix multiplication: \n        C[b, i, j, k] = sum_l A[b, i, j, l] * B[l, k]\n    \n    Args:\n        A (jax.numpy.ndarray): Input 4D tensor of shape (b, i, j, l)\n        B (jax.numpy.ndarray): Input matrix of shape (l, k)\n    \n    Returns:\n        jax.numpy.ndarray: Output 4D tensor of shape (b, i, j, k)\n    \"\"\"\n    # TODO: Replace this placeholder with your implementation.\n    # Helpful hints:\n    # - Review the tensors emitted by `get_inputs` so your function signature stays compatible.\n    # - Keep every computation inside JAX/Flax; avoid NumPy on the host to stay TPU-ready.\n    # - Capture the contraction explicitly with `jnp.einsum` so shapes stay readable.\n    # - Reuse the provided PRNG splitting strategy so shapes stay reproducible.\n    raise NotImplementedError('Implement the D tensor matrix multiplication logic')\n\n# Test code\nb = 8\ni = 256\nj = 512\nl = 256\nk = 768\n\ndef get_inputs(key):\n    \"\"\"\n    Generates random input 4D tensor and matrix.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the input tensor and matrix [A, B].\n    \"\"\"\n    key_a, key_b = jax.random.split(key)\n    A = jax.random.normal(key_a, (b, i, j, l))\n    B = jax.random.normal(key_b, (l, k))\n    return [A, B]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **D tensor matrix multiplication** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "d1c3edaa-a291-48cd-badb-dddc5090a7e7", "title": "This code implements  Min reduction over a dimension", "difficulty": "easy", "description": "### Challenge Brief\nSimple model that performs min reduction over a specific dimension. Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\nreduce_dim = 1\nbatch_size = 128\ndim1 = 4096\ndim2 = 4095\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nreduce_dim = 1\n\ndef model(x):\n    \"\"\"\n    Simple model that performs min reduction over a specific dimension.\n    \"\"\"\n    return jnp.min(x, axis=reduce_dim)\n\nbatch_size = 128\ndim1 = 4096\ndim2 = 4095\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim1, dim2))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n    \"\"\"\n    return []\n", "student_template": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nreduce_dim = 1\n\ndef model(x):\n    \"\"\"\n    Simple model that performs min reduction over a specific dimension.\n    \"\"\"\n    # TODO: Replace this placeholder with your implementation.\n    # Helpful hints:\n    # - Review the tensors emitted by `get_inputs` so your function signature stays compatible.\n    # - Keep every computation inside JAX/Flax; avoid NumPy on the host to stay TPU-ready.\n    # - Supply the right axis/keepdims pair to `jnp.min` for deterministic reductions.\n    # - Reuse the provided PRNG splitting strategy so shapes stay reproducible.\n    raise NotImplementedError('Implement the Min reduction over a dimension logic')\n\nbatch_size = 128\ndim1 = 4096\ndim2 = 4095\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim1, dim2))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n    \"\"\"\n    return []", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **Min reduction over a dimension** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "5db00e4b-0a10-45c2-92ab-1ccd0147ce1a", "title": "This code implements  LNorm", "difficulty": "medium", "description": "### Challenge Brief\nSimple model that performs L1 normalization. Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\nbatch_size = 32768\ndim = 65535\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(x):\n    \"\"\"\n    Simple model that performs L1 normalization.\n    \"\"\"\n    return x / jnp.mean(jnp.abs(x), axis=1, keepdims=True)\n\nbatch_size = 32768\n# choose dim so total <2^31\ndim = 65535\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n    \"\"\"\n    return []\n", "student_template": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(x):\n    \"\"\"\n    Simple model that performs L1 normalization.\n    \"\"\"\n    # TODO: Replace this placeholder with your implementation.\n    # Helpful hints:\n    # - Review the tensors emitted by `get_inputs` so your function signature stays compatible.\n    # - Keep every computation inside JAX/Flax; avoid NumPy on the host to stay TPU-ready.\n    # - When reducing with `jnp.mean`, pick the axis explicitly and decide whether to keep dimensions.\n    # - Apply `jnp.abs` elementwise directly on JAX arrays to stay on device.\n    # - Reuse the provided PRNG splitting strategy so shapes stay reproducible.\n    raise NotImplementedError('Implement the LNorm logic')\n\nbatch_size = 32768\n# choose dim so total <2^31\ndim = 65535\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n    \"\"\"\n    return []", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **LNorm** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "772c3649-0d45-4a0e-baa0-417b0d721d4b", "title": "This code implements  conv transposed D asymmetric input asymmetric kernel   padded", "difficulty": "hard", "description": "### Challenge Brief\nFinish implementing **conv transposed D asymmetric input asymmetric kernel padded** on TPU hardware. Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\nin_channels = 32\nout_channels = 32\nkernel_size = (3, 7)\nstride = (1, 1)\npadding = (1, 3)\nbatch_size = 8\nheight = 512\nwidth = 1024\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nin_channels = 32\nout_channels = 32\nkernel_size = (3, 7)\nstride = (1, 1)\npadding = (1, 3)\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        # PyTorch input: (N, C, H, W)\n        # JAX input: (N, H, W, C)\n        # We will generate data in JAX format.\n        return nn.ConvTranspose(features=out_channels,\n                                kernel_size=kernel_size,\n                                strides=stride,\n                                padding=[(padding[0], padding[0]), (padding[1], padding[1])],\n                                use_bias=False)(x)\n\n# Test code\nbatch_size = 8\nheight = 512\nwidth = 1024\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, height, width, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)\n", "student_template": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nin_channels = 32\nout_channels = 32\nkernel_size = (3, 7)\nstride = (1, 1)\npadding = (1, 3)\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        \"\"\"\n        Complete the conv transposed D asymmetric input asymmetric kernel padded forward pass.\n        \"\"\"\n        # TODO: Replace this placeholder with your implementation.\n        # Helpful hints:\n        # - Review the tensors emitted by `get_inputs` so your function signature stays compatible.\n        # - Keep every computation inside JAX/Flax; avoid NumPy on the host to stay TPU-ready.\n        # - Instantiate and call `nn.Conv` with the channel counts shown in the scaffold.\n        # - Instantiate `nn.ConvTranspose` with the provided strides, kernel size, and channel config.\n        # - Reuse the provided PRNG splitting strategy so shapes stay reproducible.\n        raise NotImplementedError('Implement the conv transposed D asymmetric input asymmetric kernel padded logic')\n\n# Test code\nbatch_size = 8\nheight = 512\nwidth = 1024\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, height, width, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **conv transposed D asymmetric input asymmetric kernel padded** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "8c0ec885-7887-418f-a758-8cbc21bfe4c2", "title": "This code implements  conv transposed D asymmetric input square kernel  strided padded  grouped", "difficulty": "hard", "description": "### Challenge Brief\nFinish implementing **conv transposed D asymmetric input square kernel strided padded grouped** on TPU hardware. Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\nin_channels = 32\nout_channels = 32\nkernel_size = 3\nstride = 2\npadding = 1\ngroups = 4\nbatch_size = 4\ndepth = 32\nheight = 64\nwidth = 128\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nin_channels = 32\nout_channels = 32\nkernel_size = 3\nstride = 2\npadding = 1\ngroups = 4\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        # PyTorch input: (N, C, D, H, W)\n        # JAX input: (N, D, H, W, C)\n        # We will generate data in JAX format.\n        return nn.ConvTranspose(features=out_channels,\n                                kernel_size=(kernel_size, kernel_size, kernel_size),\n                                strides=(stride, stride, stride),\n                                padding=[(padding, padding), (padding, padding), (padding, padding)],\n                                feature_group_count=groups,\n                                use_bias=False)(x)\n\n# Test code\nbatch_size = 4\ndepth = 32\nheight = 64\nwidth = 128\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, depth, height, width, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)\n", "student_template": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nin_channels = 32\nout_channels = 32\nkernel_size = 3\nstride = 2\npadding = 1\ngroups = 4\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        \"\"\"\n        Complete the conv transposed D asymmetric input square kernel strided padded grouped forward pass.\n        \"\"\"\n        # TODO: Replace this placeholder with your implementation.\n        # Helpful hints:\n        # - Review the tensors emitted by `get_inputs` so your function signature stays compatible.\n        # - Keep every computation inside JAX/Flax; avoid NumPy on the host to stay TPU-ready.\n        # - Instantiate and call `nn.Conv` with the channel counts shown in the scaffold.\n        # - Instantiate `nn.ConvTranspose` with the provided strides, kernel size, and channel config.\n        # - Reuse the provided PRNG splitting strategy so shapes stay reproducible.\n        raise NotImplementedError('Implement the conv transposed D asymmetric input square kernel strided padded grouped logic')\n\n# Test code\nbatch_size = 4\ndepth = 32\nheight = 64\nwidth = 128\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, depth, height, width, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **conv transposed D asymmetric input square kernel strided padded grouped** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "6c9eec50-3687-4f8f-9a88-e18d446fb7d3", "title": "This code implements  conv transposed D  square input  asymmetric kernel", "difficulty": "hard", "description": "### Challenge Brief\nFinish implementing **conv transposed D square input asymmetric kernel** on TPU hardware. Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\nin_channels = 32\nout_channels = 64\nkernel_depth = 3\nkernel_width = 5\nkernel_height = 5\nkernel_size = (kernel_depth, kernel_width, kernel_height)\nbatch_size = 16\ndepth = 64\nwidth = 64\nheight = 64\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nin_channels = 32\nout_channels = 64\nkernel_depth = 3\nkernel_width = 5\nkernel_height = 5\nkernel_size = (kernel_depth, kernel_width, kernel_height)\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        # PyTorch input: (N, C, D, W, H)\n        # JAX input: (N, D, W, H, C)\n        # We will generate data in JAX format.\n        return nn.ConvTranspose(features=out_channels, kernel_size=kernel_size, use_bias=False)(x)\n\n# Test code\nbatch_size = 16\ndepth = 64\nwidth = 64\nheight = 64\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, depth, width, height, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)\n", "student_template": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nin_channels = 32\nout_channels = 64\nkernel_depth = 3\nkernel_width = 5\nkernel_height = 5\nkernel_size = (kernel_depth, kernel_width, kernel_height)\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        \"\"\"\n        Complete the conv transposed D square input asymmetric kernel forward pass.\n        \"\"\"\n        # TODO: Replace this placeholder with your implementation.\n        # Helpful hints:\n        # - Review the tensors emitted by `get_inputs` so your function signature stays compatible.\n        # - Keep every computation inside JAX/Flax; avoid NumPy on the host to stay TPU-ready.\n        # - Instantiate and call `nn.Conv` with the channel counts shown in the scaffold.\n        # - Instantiate `nn.ConvTranspose` with the provided strides, kernel size, and channel config.\n        # - Reuse the provided PRNG splitting strategy so shapes stay reproducible.\n        raise NotImplementedError('Implement the conv transposed D square input asymmetric kernel logic')\n\n# Test code\nbatch_size = 16\ndepth = 64\nwidth = 64\nheight = 64\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, depth, width, height, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **conv transposed D square input asymmetric kernel** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "cb8a48e3-5cb1-4b0b-b7f1-33012b332d2f", "title": "This code implements  Max Pooling D", "difficulty": "medium", "description": "### Challenge Brief\nFinish implementing **Max Pooling D** on TPU hardware. Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\nkernel_size = 8\nstride      = 1\npadding     = 4\ndilation    = 3\nbatch_size = 64\nfeatures = 192\nsequence_length = 65536\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nkernel_size = 8\nstride      = 1\npadding     = 4\ndilation    = 3\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        # PyTorch input: (N, C, W)\n        # JAX input: (N, W, C)\n        # We will generate data in JAX format.\n        return nn.max_pool(x,\n                           window_shape=(kernel_size,),\n                           strides=(stride,),\n                           padding=[(padding, padding)],\n                           window_dilation=(dilation,))\n\nbatch_size = 64\nfeatures = 192\nsequence_length = 65536\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, sequence_length, features))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)\n", "student_template": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nkernel_size = 8\nstride      = 1\npadding     = 4\ndilation    = 3\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        \"\"\"\n        Complete the Max Pooling D forward pass.\n        \"\"\"\n        # TODO: Replace this placeholder with your implementation.\n        # Helpful hints:\n        # - Review the tensors emitted by `get_inputs` so your function signature stays compatible.\n        # - Keep every computation inside JAX/Flax; avoid NumPy on the host to stay TPU-ready.\n        # - Mirror the provided window/stride/padding/dilation hyperparameters when calling `nn.max_pool`.\n        # - Reuse the provided PRNG splitting strategy so shapes stay reproducible.\n        raise NotImplementedError('Implement the Max Pooling D logic')\n\nbatch_size = 64\nfeatures = 192\nsequence_length = 65536\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, sequence_length, features))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **Max Pooling D** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "bb1fc4c0-119d-4a10-b9af-c56da868cbac", "title": "This code implements  Sum reduction over a dimension", "difficulty": "easy", "description": "### Challenge Brief\nSimple model that performs sum reduction over a specified dimension. Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\nreduce_dim = 1\nbatch_size = 128\ndim1 = 4096\ndim2 = 4095\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nreduce_dim = 1\n\ndef model(x):\n    \"\"\"\n    Simple model that performs sum reduction over a specified dimension.\n    \"\"\"\n    return jnp.sum(x, axis=reduce_dim, keepdims=True)\n\nbatch_size = 128\ndim1 = 4096\ndim2 = 4095\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim1, dim2))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n    \"\"\"\n    return []\n", "student_template": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nreduce_dim = 1\n\ndef model(x):\n    \"\"\"\n    Simple model that performs sum reduction over a specified dimension.\n    \"\"\"\n    # TODO: Replace this placeholder with your implementation.\n    # Helpful hints:\n    # - Review the tensors emitted by `get_inputs` so your function signature stays compatible.\n    # - Keep every computation inside JAX/Flax; avoid NumPy on the host to stay TPU-ready.\n    # - Leverage `jnp.sum` across the axes hinted at in the scaffold rather than flattening everything.\n    # - Reuse the provided PRNG splitting strategy so shapes stay reproducible.\n    raise NotImplementedError('Implement the Sum reduction over a dimension logic')\n\nbatch_size = 128\ndim1 = 4096\ndim2 = 4095\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim1, dim2))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n    \"\"\"\n    return []", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **Sum reduction over a dimension** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "3a0fb29e-56aa-48dc-b2c6-3a95bb8c9f4e", "title": "This code implements  InstanceNorm", "difficulty": "medium", "description": "### Challenge Brief\nSimple model that performs Instance Normalization. Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\nbatch_size = 112  # heavier workload\nfeatures = 64\ndim1 = 512\ndim2 = 512\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs Instance Normalization.\n    \"\"\"\n    @nn.compact\n    def __call__(self, x):\n        \"\"\"\n        Applies Instance Normalization to the input tensor.\n\n        Args:\n            x (jax.numpy.ndarray): Input tensor of shape (batch_size, dim1, dim2, features).\n\n        Returns:\n            jax.numpy.ndarray: Output tensor with Instance Normalization applied, same shape as input.\n        \"\"\"\n        return nn.InstanceNorm()(x)\n\nbatch_size = 112  # heavier workload\nfeatures = 64\ndim1 = 512\ndim2 = 512\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim1, dim2, features))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)\n", "student_template": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs Instance Normalization.\n    \"\"\"\n    @nn.compact\n    def __call__(self, x):\n        \"\"\"\n        Applies Instance Normalization to the input tensor.\n        \n        Args:\n            x (jax.numpy.ndarray): Input tensor of shape (batch_size, dim1, dim2, features).\n        \n        Returns:\n            jax.numpy.ndarray: Output tensor with Instance Normalization applied, same shape as input.\n        \"\"\"\n        # TODO: Replace this placeholder with your implementation.\n        # Helpful hints:\n        # - Review the tensors emitted by `get_inputs` so your function signature stays compatible.\n        # - Keep every computation inside JAX/Flax; avoid NumPy on the host to stay TPU-ready.\n        # - Call `nn.InstanceNorm` on the channel-last tensors generated in the inputs.\n        # - Reuse the provided PRNG splitting strategy so shapes stay reproducible.\n        raise NotImplementedError('Implement the InstanceNorm logic')\n\nbatch_size = 112  # heavier workload\nfeatures = 64\ndim1 = 512\ndim2 = 512\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim1, dim2, features))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **InstanceNorm** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "1192b589-bc37-4804-8d16-6bb4293d2f1c", "title": "This code implements  conv transposed D  asymmetric input  asymmetric kernel", "difficulty": "hard", "description": "### Challenge Brief\nFinish implementing **conv transposed D asymmetric input asymmetric kernel** on TPU hardware. Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\nin_channels = 64\nout_channels = 128\nkernel_size = (3, 5)\nbatch_size = 64\nheight_in = 128\nwidth_in = 256\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nin_channels = 64\nout_channels = 128\nkernel_size = (3, 5)\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        # PyTorch input: (N, C, H, W)\n        # JAX input: (N, H, W, C)\n        # We will generate data in JAX format.\n        return nn.ConvTranspose(features=out_channels, kernel_size=kernel_size, use_bias=False)(x)\n\n# Test code\nbatch_size = 64\nheight_in = 128\nwidth_in = 256\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, height_in, width_in, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)\n", "student_template": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nin_channels = 64\nout_channels = 128\nkernel_size = (3, 5)\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        \"\"\"\n        Complete the conv transposed D asymmetric input asymmetric kernel forward pass.\n        \"\"\"\n        # TODO: Replace this placeholder with your implementation.\n        # Helpful hints:\n        # - Review the tensors emitted by `get_inputs` so your function signature stays compatible.\n        # - Keep every computation inside JAX/Flax; avoid NumPy on the host to stay TPU-ready.\n        # - Instantiate and call `nn.Conv` with the channel counts shown in the scaffold.\n        # - Instantiate `nn.ConvTranspose` with the provided strides, kernel size, and channel config.\n        # - Reuse the provided PRNG splitting strategy so shapes stay reproducible.\n        raise NotImplementedError('Implement the conv transposed D asymmetric input asymmetric kernel logic')\n\n# Test code\nbatch_size = 64\nheight_in = 128\nwidth_in = 256\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, height_in, width_in, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **conv transposed D asymmetric input asymmetric kernel** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "95b8859d-2e92-4335-92fc-24611e8a90aa", "title": "This code implements  Max Pooling D", "difficulty": "medium", "description": "### Challenge Brief\nFinish implementing **Max Pooling D** on TPU hardware. Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\nkernel_size = 4\nstride = 1\npadding = 1\ndilation = 1\nbatch_size = 32\nchannels = 64\nheight = 512\nwidth = 512\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nkernel_size = 4\nstride = 1\npadding = 1\ndilation = 1\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        # PyTorch input: (N, C, H, W)\n        # JAX input: (N, H, W, C)\n        # We will generate data in JAX format.\n        return nn.max_pool(x,\n                           window_shape=(kernel_size, kernel_size),\n                           strides=(stride, stride),\n                           padding=[(padding, padding), (padding, padding)],\n                           window_dilation=(dilation, dilation))\n\nbatch_size = 32\nchannels = 64\nheight = 512\nwidth = 512\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, height, width, channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)\n", "student_template": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nkernel_size = 4\nstride = 1\npadding = 1\ndilation = 1\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        \"\"\"\n        Complete the Max Pooling D forward pass.\n        \"\"\"\n        # TODO: Replace this placeholder with your implementation.\n        # Helpful hints:\n        # - Review the tensors emitted by `get_inputs` so your function signature stays compatible.\n        # - Keep every computation inside JAX/Flax; avoid NumPy on the host to stay TPU-ready.\n        # - Mirror the provided window/stride/padding/dilation hyperparameters when calling `nn.max_pool`.\n        # - Reuse the provided PRNG splitting strategy so shapes stay reproducible.\n        raise NotImplementedError('Implement the Max Pooling D logic')\n\nbatch_size = 32\nchannels = 64\nheight = 512\nwidth = 512\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, height, width, channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **Max Pooling D** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "1c1e9146-5546-4978-bab6-fb112db149e3", "title": "This code implements  RMSNorm", "difficulty": "medium", "description": "### Challenge Brief\nSimple model that performs RMS Normalization. Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\nbatch_size = 112\nfeatures = 64\ndim1 = 512\ndim2 = 512\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs RMS Normalization.\n    \"\"\"\n    @nn.compact\n    def __call__(self, x):\n        \"\"\"\n        Applies RMS Normalization to the input tensor.\n\n        Args:\n            x (jax.numpy.ndarray): Input tensor of shape (batch_size, dim1, dim2, features).\n\n        Returns:\n            jax.numpy.ndarray: Output tensor with RMS Normalization applied, same shape as input.\n        \"\"\"\n        return nn.RMSNorm()(x)\n\nbatch_size = 112\nfeatures = 64\ndim1 = 512\ndim2 = 512\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim1, dim2, features))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)\n", "student_template": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs RMS Normalization.\n    \"\"\"\n    @nn.compact\n    def __call__(self, x):\n        \"\"\"\n        Applies RMS Normalization to the input tensor.\n        \n        Args:\n            x (jax.numpy.ndarray): Input tensor of shape (batch_size, dim1, dim2, features).\n        \n        Returns:\n            jax.numpy.ndarray: Output tensor with RMS Normalization applied, same shape as input.\n        \"\"\"\n        # TODO: Replace this placeholder with your implementation.\n        # Helpful hints:\n        # - Review the tensors emitted by `get_inputs` so your function signature stays compatible.\n        # - Keep every computation inside JAX/Flax; avoid NumPy on the host to stay TPU-ready.\n        # - Normalize activations with `nn.RMSNorm` before the projection.\n        # - Reuse the provided PRNG splitting strategy so shapes stay reproducible.\n        raise NotImplementedError('Implement the RMSNorm logic')\n\nbatch_size = 112\nfeatures = 64\ndim1 = 512\ndim2 = 512\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim1, dim2, features))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **RMSNorm** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "fe0ded16-1d59-45ae-a1ca-e169ed6615e2", "title": "This code implements  FrobeniusNorm", "difficulty": "medium", "description": "### Challenge Brief\nSimple model that performs Frobenius norm normalization. Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\nbatch_size = 112\nfeatures = 64\ndim1 = 512\ndim2 = 512\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(x):\n    \"\"\"\n    Simple model that performs Frobenius norm normalization.\n    \"\"\"\n    norm = jnp.linalg.norm(x, ord='fro')\n    return x / norm\n\nbatch_size = 112\nfeatures = 64\ndim1 = 512\ndim2 = 512\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, features, dim1, dim2))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n    \"\"\"\n    return []\n", "student_template": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(x):\n    \"\"\"\n    Simple model that performs Frobenius norm normalization.\n    \"\"\"\n    # TODO: Replace this placeholder with your implementation.\n    # Helpful hints:\n    # - Review the tensors emitted by `get_inputs` so your function signature stays compatible.\n    # - Keep every computation inside JAX/Flax; avoid NumPy on the host to stay TPU-ready.\n    # - Stick with `jnp.linalg` routines to stay inside JAX for decompositions and solves.\n    # - Reuse the provided PRNG splitting strategy so shapes stay reproducible.\n    raise NotImplementedError('Implement the FrobeniusNorm logic')\n\nbatch_size = 112\nfeatures = 64\ndim1 = 512\ndim2 = 512\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, features, dim1, dim2))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n    \"\"\"\n    return []", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **FrobeniusNorm** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "830bb310-9a3e-4d25-ba4c-761390738b7b", "title": "This code implements  conv transposed D  asymmetric input  square kernel", "difficulty": "hard", "description": "### Challenge Brief\nFinish implementing **conv transposed D asymmetric input square kernel** on TPU hardware. Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\nin_channels = 48\nout_channels = 24\nkernel_size = 3\nbatch_size = 8\ndepth = 96\nheight = 96\nwidth = 96\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nin_channels = 48\nout_channels = 24\nkernel_size = 3\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        # PyTorch input: (N, C, D, H, W)\n        # JAX input: (N, D, H, W, C)\n        # We will generate data in JAX format.\n        return nn.ConvTranspose(features=out_channels, kernel_size=(kernel_size, kernel_size, kernel_size), use_bias=False)(x)\n\n# Test code\nbatch_size = 8\ndepth = 96\nheight = 96\nwidth = 96\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, depth, height, width, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)\n", "student_template": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nin_channels = 48\nout_channels = 24\nkernel_size = 3\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        \"\"\"\n        Complete the conv transposed D asymmetric input square kernel forward pass.\n        \"\"\"\n        # TODO: Replace this placeholder with your implementation.\n        # Helpful hints:\n        # - Review the tensors emitted by `get_inputs` so your function signature stays compatible.\n        # - Keep every computation inside JAX/Flax; avoid NumPy on the host to stay TPU-ready.\n        # - Instantiate and call `nn.Conv` with the channel counts shown in the scaffold.\n        # - Instantiate `nn.ConvTranspose` with the provided strides, kernel size, and channel config.\n        # - Reuse the provided PRNG splitting strategy so shapes stay reproducible.\n        raise NotImplementedError('Implement the conv transposed D asymmetric input square kernel logic')\n\n# Test code\nbatch_size = 8\ndepth = 96\nheight = 96\nwidth = 96\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, depth, height, width, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **conv transposed D asymmetric input square kernel** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "25ae69ad-3a11-431d-93b5-233b89d75bef", "title": "This code implements  Average Pooling D", "difficulty": "medium", "description": "### Challenge Brief\nFinish implementing **Average Pooling D** on TPU hardware. Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\nkernel_size = 8\nstride = 1\npadding = 4\nbatch_size = 64\nin_channels = 128\ninput_length = 65536\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nkernel_size = 8\nstride = 1\npadding = 4\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        # PyTorch input: (N, C, W)\n        # JAX input: (N, W, C)\n        # We will generate data in JAX format.\n        return nn.avg_pool(x,\n                           window_shape=(kernel_size,),\n                           strides=(stride,),\n                           padding=[(padding, padding)])\n\nbatch_size = 64\nin_channels = 128\ninput_length = 65536\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, input_length, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)\n", "student_template": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nkernel_size = 8\nstride = 1\npadding = 4\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        \"\"\"\n        Complete the Average Pooling D forward pass.\n        \"\"\"\n        # TODO: Replace this placeholder with your implementation.\n        # Helpful hints:\n        # - Review the tensors emitted by `get_inputs` so your function signature stays compatible.\n        # - Keep every computation inside JAX/Flax; avoid NumPy on the host to stay TPU-ready.\n        # - Mirror the provided window/stride/padding hyperparameters when calling `nn.avg_pool`.\n        # - Reuse the provided PRNG splitting strategy so shapes stay reproducible.\n        raise NotImplementedError('Implement the Average Pooling D logic')\n\nbatch_size = 64\nin_channels = 128\ninput_length = 65536\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, input_length, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **Average Pooling D** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "d5393f62-dea2-4feb-8972-262d4668106e", "title": "This code implements  Max reduction over a dimension", "difficulty": "easy", "description": "### Challenge Brief\nSimple model that performs Max reduction over a specific dimension. Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\nreduce_dim = 1\nbatch_size = 128\ndim1 = 4096\ndim2 = 4095\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nreduce_dim = 1\n\ndef model(x):\n    \"\"\"\n    Simple model that performs Max reduction over a specific dimension.\n    \"\"\"\n    return jnp.max(x, axis=reduce_dim)\n\nbatch_size = 128\ndim1 = 4096\ndim2 = 4095\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim1, dim2))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n    \"\"\"\n    return []\n", "student_template": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nreduce_dim = 1\n\ndef model(x):\n    \"\"\"\n    Simple model that performs Max reduction over a specific dimension.\n    \"\"\"\n    # TODO: Replace this placeholder with your implementation.\n    # Helpful hints:\n    # - Review the tensors emitted by `get_inputs` so your function signature stays compatible.\n    # - Keep every computation inside JAX/Flax; avoid NumPy on the host to stay TPU-ready.\n    # - Match the intended axes when calling `jnp.max` to avoid collapsing the wrong dimension.\n    # - Reuse the provided PRNG splitting strategy so shapes stay reproducible.\n    raise NotImplementedError('Implement the Max reduction over a dimension logic')\n\nbatch_size = 128\ndim1 = 4096\ndim2 = 4095\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim1, dim2))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n    \"\"\"\n    return []", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **Max reduction over a dimension** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "0ebae6a9-f3d3-41d6-8cf5-0e02768c8e23", "title": "This code implements  Average Pooling D", "difficulty": "medium", "description": "### Challenge Brief\nFinish implementing **Average Pooling D** on TPU hardware. Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\nkernel_size = 3\nstride = 2\npadding = 1\nbatch_size = 16\nchannels = 32\ndepth = 128\nheight = 128\nwidth = 256\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nkernel_size = 3\nstride = 2\npadding = 1\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        # PyTorch input: (N, C, D, H, W)\n        # JAX input: (N, D, H, W, C)\n        # We will generate data in JAX format.\n        return nn.avg_pool(x,\n                           window_shape=(kernel_size, kernel_size, kernel_size),\n                           strides=(stride, stride, stride),\n                           padding=[(padding, padding), (padding, padding), (padding, padding)])\n\nbatch_size = 16\nchannels = 32\ndepth = 128\nheight = 128\nwidth = 256\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, depth, height, width, channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)\n", "student_template": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nkernel_size = 3\nstride = 2\npadding = 1\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        \"\"\"\n        Complete the Average Pooling D forward pass.\n        \"\"\"\n        # TODO: Replace this placeholder with your implementation.\n        # Helpful hints:\n        # - Review the tensors emitted by `get_inputs` so your function signature stays compatible.\n        # - Keep every computation inside JAX/Flax; avoid NumPy on the host to stay TPU-ready.\n        # - Mirror the provided window/stride/padding hyperparameters when calling `nn.avg_pool`.\n        # - Reuse the provided PRNG splitting strategy so shapes stay reproducible.\n        raise NotImplementedError('Implement the Average Pooling D logic')\n\nbatch_size = 16\nchannels = 32\ndepth = 128\nheight = 128\nwidth = 256\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, depth, height, width, channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **Average Pooling D** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "968b9f07-e3c7-4858-8c5e-490f0f4e5016", "title": "This code implements process files", "difficulty": "easy", "description": "### Challenge Brief\nFinish implementing **process files** on TPU hardware. Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\noutput_dir = \"level1_jax_jsonl\"\noutput_file = os.path.join(output_dir, \"level1_jax.jsonl\")\nfiles = [f for f in os.listdir('.') if f.endswith('.py')]\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import json\nimport uuid\nimport os\n\ndef create_jsonl_entry(filename, content):\n    description = \"This code implements {}.\".format(filename.replace('_', ' ').replace('.py', ''))\n    docstring = \"\"\n    try:\n        # A simple way to extract a docstring from the first function if it exists\n        # This is not a robust parser.\n        start = content.find('\"\"\"')\n        if start != -1:\n            end = content.find('\"\"\"', start + 3)\n            if end != -1:\n                docstring = content[start+3:end].strip()\n                description += \"\\n\\n\" + docstring\n    except Exception as e:\n        pass\n\n    return {\n        \"id\": str(uuid.uuid4()),\n        \"code\": content,\n        \"description\": description,\n        \"difficulty\": 3, # Default difficulty\n    }\n\noutput_dir = \"level1_jax_jsonl\"\noutput_file = os.path.join(output_dir, \"level1_jax.jsonl\")\n\n# List all python files\nfiles = [f for f in os.listdir('.') if f.endswith('.py')]\n\nwith open(output_file, 'w') as f:\n    for filename in files:\n        # a file may not be a python file\n        if not filename.endswith('.py'):\n            continue\n        with open(filename, 'r') as py_file:\n            content = py_file.read()\n            entry = create_jsonl_entry(filename, content)\n            f.write(json.dumps(entry) + '\\n')\n\nprint(\"Processed {} files and created {}\".format(len(files), output_file))", "student_template": "import json\nimport uuid\nimport os\n\ndef create_jsonl_entry(filename, content):\n    description = \"This code implements {}.\".format(filename.replace('_', ' ').replace('.py', ''))\n    docstring = \"\"\n    try:\n        # A simple way to extract a docstring from the first function if it exists\n        # This is not a robust parser.\n        start = content.find('\"\"\"')\n        if start != -1:\n            end = content.find('\"\"\"', start + 3)\n            if end != -1:\n                docstring = content[start+3:end].strip()\n                description += \"\\n\\n\" + docstring\n    except Exception as e:\n        pass\n\n    return {\n        \"id\": str(uuid.uuid4()),\n        \"code\": content,\n        \"description\": description,\n        \"difficulty\": 3, # Default difficulty\n    }\n\noutput_dir = \"level1_jax_jsonl\"\noutput_file = os.path.join(output_dir, \"level1_jax.jsonl\")\n\n# List all python files\nfiles = [f for f in os.listdir('.') if f.endswith('.py')]\n\nwith open(output_file, 'w') as f:\n    for filename in files:\n        # a file may not be a python file\n        if not filename.endswith('.py'):\n            continue\n        with open(filename, 'r') as py_file:\n            content = py_file.read()\n            entry = create_jsonl_entry(filename, content)\n            f.write(json.dumps(entry) + '\\n')\n\nprint(\"Processed {} files and created {}\".format(len(files), output_file))", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **process files** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "0aa10606-17d3-4bce-bf7e-d3fa9df2cf7b", "title": "This code implements  conv standard D  square input  asymmetric kernel", "difficulty": "hard", "description": "### Challenge Brief\nFinish implementing **conv standard D square input asymmetric kernel** on TPU hardware. Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\nin_channels = 32\nout_channels = 64\nkernel_size = (5, 9)\nbatch_size = 8\nwidth = 512\nheight = 512\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nin_channels = 32\nout_channels = 64\nkernel_size = (5, 9)\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        # PyTorch input: (N, C, H, W)\n        # JAX input: (N, H, W, C)\n        # We will generate data in JAX format.\n        return nn.Conv(features=out_channels, kernel_size=kernel_size, use_bias=False)(x)\n\n# Test code\nbatch_size = 8\nwidth = 512\nheight = 512\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, height, width, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)\n", "student_template": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nin_channels = 32\nout_channels = 64\nkernel_size = (5, 9)\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        \"\"\"\n        Complete the conv standard D square input asymmetric kernel forward pass.\n        \"\"\"\n        # TODO: Replace this placeholder with your implementation.\n        # Helpful hints:\n        # - Review the tensors emitted by `get_inputs` so your function signature stays compatible.\n        # - Keep every computation inside JAX/Flax; avoid NumPy on the host to stay TPU-ready.\n        # - Instantiate and call `nn.Conv` with the channel counts shown in the scaffold.\n        # - Reuse the provided PRNG splitting strategy so shapes stay reproducible.\n        raise NotImplementedError('Implement the conv standard D square input asymmetric kernel logic')\n\n# Test code\nbatch_size = 8\nwidth = 512\nheight = 512\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, height, width, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **conv standard D square input asymmetric kernel** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "5693bd0b-9732-4222-8f0f-721b85713660", "title": "This code implements  HardSigmoid", "difficulty": "easy", "description": "### Challenge Brief\nSimple model that performs a HardSigmoid activation. Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\nbatch_size = 4096\ndim = 393216\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(x):\n    \"\"\"\n    Simple model that performs a HardSigmoid activation.\n    \"\"\"\n    return nn.hard_sigmoid(x)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the input tensor [x].\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []\n", "student_template": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(x):\n    \"\"\"\n    Simple model that performs a HardSigmoid activation.\n    \"\"\"\n    # TODO: Replace this placeholder with your implementation.\n    # Helpful hints:\n    # - Review the tensors emitted by `get_inputs` so your function signature stays compatible.\n    # - Keep every computation inside JAX/Flax; avoid NumPy on the host to stay TPU-ready.\n    # - Call `nn.hard_sigmoid` when the scaffold shows that activation.\n    # - Reuse the provided PRNG splitting strategy so shapes stay reproducible.\n    raise NotImplementedError('Implement the HardSigmoid logic')\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the input tensor [x].\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **HardSigmoid** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "f8626368-0bab-45be-a9c5-c1b6dd652606", "title": "This code implements  LNorm", "difficulty": "medium", "description": "### Challenge Brief\nSimple model that performs L2 normalization. Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\nbatch_size = 32768\ndim = 65535\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(x):\n    \"\"\"\n    Simple model that performs L2 normalization.\n    \"\"\"\n    return x / jnp.linalg.norm(x, ord=2, axis=1, keepdims=True)\n\nbatch_size = 32768\n# choose dim so total <2^31\ndim = 65535\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n    \"\"\"\n    return []\n", "student_template": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(x):\n    \"\"\"\n    Simple model that performs L2 normalization.\n    \"\"\"\n    # TODO: Replace this placeholder with your implementation.\n    # Helpful hints:\n    # - Review the tensors emitted by `get_inputs` so your function signature stays compatible.\n    # - Keep every computation inside JAX/Flax; avoid NumPy on the host to stay TPU-ready.\n    # - Stick with `jnp.linalg` routines to stay inside JAX for decompositions and solves.\n    # - Reuse the provided PRNG splitting strategy so shapes stay reproducible.\n    raise NotImplementedError('Implement the LNorm logic')\n\nbatch_size = 32768\n# choose dim so total <2^31\ndim = 65535\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n    \"\"\"\n    return []", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **LNorm** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "12e2f9ab-af66-4668-93c1-82ed1f659508", "title": "This code implements  conv transposed D asymmetric input asymmetric kernel strided  grouped    padded    dilated", "difficulty": "hard", "description": "### Challenge Brief\nFinish implementing **conv transposed D asymmetric input asymmetric kernel strided grouped padded dilated** on TPU hardware. Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\nin_channels = 32\nout_channels = 64\nkernel_size = (3, 5)\nstride = (2, 3)\npadding = (1, 2)\ndilation = (2, 1)\ngroups = 4\nbatch_size = 16\nheight = 128\nwidth = 256\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nin_channels = 32\nout_channels = 64\nkernel_size = (3, 5)\nstride = (2, 3)\npadding = (1, 2)\ndilation = (2, 1)\ngroups = 4\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        # PyTorch input: (N, C, H, W)\n        # JAX input: (N, H, W, C)\n        # We will generate data in JAX format.\n        return nn.ConvTranspose(features=out_channels,\n                                kernel_size=kernel_size,\n                                strides=stride,\n                                padding=[(padding[0], padding[0]), (padding[1], padding[1])],\n                                kernel_dilation=dilation,\n                                feature_group_count=groups,\n                                use_bias=False)(x)\n\n# Test code\nbatch_size = 16\nheight = 128\nwidth = 256\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, height, width, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)\n", "student_template": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nin_channels = 32\nout_channels = 64\nkernel_size = (3, 5)\nstride = (2, 3)\npadding = (1, 2)\ndilation = (2, 1)\ngroups = 4\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        \"\"\"\n        Complete the conv transposed D asymmetric input asymmetric kernel strided grouped padded dilated forward pass.\n        \"\"\"\n        # TODO: Replace this placeholder with your implementation.\n        # Helpful hints:\n        # - Review the tensors emitted by `get_inputs` so your function signature stays compatible.\n        # - Keep every computation inside JAX/Flax; avoid NumPy on the host to stay TPU-ready.\n        # - Instantiate and call `nn.Conv` with the channel counts shown in the scaffold.\n        # - Instantiate `nn.ConvTranspose` with the provided strides, kernel size, and channel config.\n        # - Reuse the provided PRNG splitting strategy so shapes stay reproducible.\n        raise NotImplementedError('Implement the conv transposed D asymmetric input asymmetric kernel strided grouped padded dilated logic')\n\n# Test code\nbatch_size = 16\nheight = 128\nwidth = 256\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, height, width, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **conv transposed D asymmetric input asymmetric kernel strided grouped padded dilated** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "a1659372-385e-4687-b5bc-d0cecf22429b", "title": "This code implements  Matmul for upper triangular matrices", "difficulty": "easy", "description": "### Challenge Brief\nSimple model that performs matrix multiplication (C = A * B) for upper triangular matrices. Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\nN = 4096\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(A, B):\n    \"\"\"\n    Simple model that performs matrix multiplication (C = A * B) for upper triangular matrices.\n    \"\"\"\n    return jnp.triu(jnp.matmul(A, B))\n\nN = 4096\n\ndef get_inputs(key):\n    \"\"\"\n    Generates upper triangular matrices for testing.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing two upper triangular matrices of shape (N, N).\n    \"\"\"\n    key_a, key_b = jax.random.split(key)\n    A = jnp.triu(jax.random.normal(key_a, (N, N)))\n    B = jnp.triu(jax.random.normal(key_b, (N, N)))\n    return [A, B]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No specific initialization inputs are needed for this model.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []\n", "student_template": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(A, B):\n    \"\"\"\n    Simple model that performs matrix multiplication (C = A * B) for upper triangular matrices.\n    \"\"\"\n    # TODO: Replace this placeholder with your implementation.\n    # Helpful hints:\n    # - Review the tensors emitted by `get_inputs` so your function signature stays compatible.\n    # - Keep every computation inside JAX/Flax; avoid NumPy on the host to stay TPU-ready.\n    # - Use `jnp.matmul` (or the `@` operator) to perform the core multiply without leaving JAX.\n    # - Use `jnp.triu` to zero-out everything below the diagonal before continuing.\n    # - Reuse the provided PRNG splitting strategy so shapes stay reproducible.\n    raise NotImplementedError('Implement the Matmul for upper triangular matrices logic')\n\nN = 4096\n\ndef get_inputs(key):\n    \"\"\"\n    Generates upper triangular matrices for testing.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing two upper triangular matrices of shape (N, N).\n    \"\"\"\n    key_a, key_b = jax.random.split(key)\n    A = jnp.triu(jax.random.normal(key_a, (N, N)))\n    B = jnp.triu(jax.random.normal(key_b, (N, N)))\n    return [A, B]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No specific initialization inputs are needed for this model.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **Matmul for upper triangular matrices** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "1b66a378-6489-4dde-8303-79cd151e7c74", "title": "This code implements  HardTanh", "difficulty": "easy", "description": "### Challenge Brief\nSimple model that performs a HardTanh activation. Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\nbatch_size = 4096\ndim = 393216\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(x):\n    \"\"\"\n    Simple model that performs a HardTanh activation.\n    \"\"\"\n    return nn.hard_tanh(x)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the input tensor [x].\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []\n", "student_template": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(x):\n    \"\"\"\n    Simple model that performs a HardTanh activation.\n    \"\"\"\n    # TODO: Replace this placeholder with your implementation.\n    # Helpful hints:\n    # - Review the tensors emitted by `get_inputs` so your function signature stays compatible.\n    # - Keep every computation inside JAX/Flax; avoid NumPy on the host to stay TPU-ready.\n    # - Call `nn.hard_tanh` to clip activations per the template.\n    # - Reuse the provided PRNG splitting strategy so shapes stay reproducible.\n    raise NotImplementedError('Implement the HardTanh logic')\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the input tensor [x].\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **HardTanh** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "1994f5b1-b723-4a74-8644-b6be01f9863d", "title": "This code implements  Mean reduction over a dimension", "difficulty": "easy", "description": "### Challenge Brief\nSimple model that performs mean reduction over a specific dimension. Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\nreduce_dim = 1\nbatch_size = 128\ndim1 = 4096\ndim2 = 4095\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nreduce_dim = 1\n\ndef model(x):\n    \"\"\"\n    Simple model that performs mean reduction over a specific dimension.\n    \"\"\"\n    return jnp.mean(x, axis=reduce_dim)\n\nbatch_size = 128\ndim1 = 4096\ndim2 = 4095\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim1, dim2))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n    \"\"\"\n    return []\n", "student_template": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nreduce_dim = 1\n\ndef model(x):\n    \"\"\"\n    Simple model that performs mean reduction over a specific dimension.\n    \"\"\"\n    # TODO: Replace this placeholder with your implementation.\n    # Helpful hints:\n    # - Review the tensors emitted by `get_inputs` so your function signature stays compatible.\n    # - Keep every computation inside JAX/Flax; avoid NumPy on the host to stay TPU-ready.\n    # - When reducing with `jnp.mean`, pick the axis explicitly and decide whether to keep dimensions.\n    # - Reuse the provided PRNG splitting strategy so shapes stay reproducible.\n    raise NotImplementedError('Implement the Mean reduction over a dimension logic')\n\nbatch_size = 128\ndim1 = 4096\ndim2 = 4095\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim1, dim2))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n    \"\"\"\n    return []", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **Mean reduction over a dimension** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "9107092b-636f-4c1a-a965-a12da3e1e66a", "title": "This code implements  LayerNorm", "difficulty": "medium", "description": "### Challenge Brief\nSimple model that performs Layer Normalization. Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\nbatch_size = 16\nfeatures = 64\ndim1 = 256\ndim2 = 256\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs Layer Normalization.\n    \"\"\"\n    @nn.compact\n    def __call__(self, x):\n        \"\"\"\n        Applies Layer Normalization to the input tensor.\n\n        Args:\n            x (jax.numpy.ndarray): Input tensor of shape (batch_size, features, dim1, dim2).\n\n        Returns:\n            jax.numpy.ndarray: Output tensor with Layer Normalization applied, same shape as input.\n        \"\"\"\n        original_shape = x.shape\n        reshaped_x = x.reshape(original_shape[0], -1)\n        normalized_x = nn.LayerNorm()(reshaped_x)\n        return normalized_x.reshape(original_shape)\n\nbatch_size = 16\nfeatures = 64\ndim1 = 256\ndim2 = 256\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, features, dim1, dim2))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)\n", "student_template": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs Layer Normalization.\n    \"\"\"\n    @nn.compact\n    def __call__(self, x):\n        \"\"\"\n        Applies Layer Normalization to the input tensor.\n        \n        Args:\n            x (jax.numpy.ndarray): Input tensor of shape (batch_size, features, dim1, dim2).\n        \n        Returns:\n            jax.numpy.ndarray: Output tensor with Layer Normalization applied, same shape as input.\n        \"\"\"\n        # TODO: Replace this placeholder with your implementation.\n        # Helpful hints:\n        # - Review the tensors emitted by `get_inputs` so your function signature stays compatible.\n        # - Keep every computation inside JAX/Flax; avoid NumPy on the host to stay TPU-ready.\n        # - Apply `nn.LayerNorm` over the final dimension(s) specified in the template.\n        # - Reuse the provided PRNG splitting strategy so shapes stay reproducible.\n        raise NotImplementedError('Implement the LayerNorm logic')\n\nbatch_size = 16\nfeatures = 64\ndim1 = 256\ndim2 = 256\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, features, dim1, dim2))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **LayerNorm** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "7e12a6aa-ccd1-4876-aff9-37c51bb37e72", "title": "This code implements  BatchNorm", "difficulty": "medium", "description": "### Challenge Brief\nSimple model that performs Batch Normalization. Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\nbatch_size = 64\nfeatures = 64\ndim1 = 512\ndim2 = 512\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs Batch Normalization.\n    \"\"\"\n    @nn.compact\n    def __call__(self, x):\n        \"\"\"\n        Applies Batch Normalization to the input tensor.\n\n        Args:\n            x (jax.numpy.ndarray): Input tensor of shape (batch_size, dim1, dim2, features).\n\n        Returns:\n            jax.numpy.ndarray: Output tensor with Batch Normalization applied, same shape as input.\n        \"\"\"\n        return nn.BatchNorm(use_running_average=True)(x)\n\nbatch_size = 64\nfeatures = 64\ndim1 = 512\ndim2 = 512\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim1, dim2, features))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)\n", "student_template": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs Batch Normalization.\n    \"\"\"\n    @nn.compact\n    def __call__(self, x):\n        \"\"\"\n        Applies Batch Normalization to the input tensor.\n        \n        Args:\n            x (jax.numpy.ndarray): Input tensor of shape (batch_size, dim1, dim2, features).\n        \n        Returns:\n            jax.numpy.ndarray: Output tensor with Batch Normalization applied, same shape as input.\n        \"\"\"\n        # TODO: Replace this placeholder with your implementation.\n        # Helpful hints:\n        # - Review the tensors emitted by `get_inputs` so your function signature stays compatible.\n        # - Keep every computation inside JAX/Flax; avoid NumPy on the host to stay TPU-ready.\n        # - Leverage `nn.BatchNorm` inside `@nn.compact` and track the `use_running_average` flag.\n        # - Reuse the provided PRNG splitting strategy so shapes stay reproducible.\n        raise NotImplementedError('Implement the BatchNorm logic')\n\nbatch_size = 64\nfeatures = 64\ndim1 = 512\ndim2 = 512\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim1, dim2, features))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **BatchNorm** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "867ce7f5-8948-4b06-8612-2a40bd6ee906", "title": "This code implements  conv standard D  square input  square kernel", "difficulty": "hard", "description": "### Challenge Brief\nFinish implementing **conv standard D square input square kernel** on TPU hardware. Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\nin_channels = 16\nout_channels = 128\nkernel_size = 3\nbatch_size = 16\nwidth = 1024\nheight = 1024\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nin_channels = 16\nout_channels = 128\nkernel_size = 3\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        # PyTorch input: (N, C, H, W)\n        # JAX input: (N, H, W, C)\n        # We will generate data in JAX format.\n        return nn.Conv(features=out_channels, kernel_size=(kernel_size, kernel_size), use_bias=False)(x)\n\n# Test code\nbatch_size = 16\nwidth = 1024\nheight = 1024\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, height, width, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)\n", "student_template": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nin_channels = 16\nout_channels = 128\nkernel_size = 3\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        \"\"\"\n        Complete the conv standard D square input square kernel forward pass.\n        \"\"\"\n        # TODO: Replace this placeholder with your implementation.\n        # Helpful hints:\n        # - Review the tensors emitted by `get_inputs` so your function signature stays compatible.\n        # - Keep every computation inside JAX/Flax; avoid NumPy on the host to stay TPU-ready.\n        # - Instantiate and call `nn.Conv` with the channel counts shown in the scaffold.\n        # - Reuse the provided PRNG splitting strategy so shapes stay reproducible.\n        raise NotImplementedError('Implement the conv standard D square input square kernel logic')\n\n# Test code\nbatch_size = 16\nwidth = 1024\nheight = 1024\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, height, width, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **conv standard D square input square kernel** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "7c0eafc2-f2bf-4ef7-95bf-aea26ba7e4b3", "title": "This code implements  conv standard D  square input  asymmetric kernel", "difficulty": "hard", "description": "### Challenge Brief\nFinish implementing **conv standard D square input asymmetric kernel** on TPU hardware. Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\nin_channels = 3\nout_channels = 64\nkernel_size = (3, 5, 7)\nbatch_size = 16\nwidth = 64\nheight = 64\ndepth = 64\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nin_channels = 3\nout_channels = 64\nkernel_size = (3, 5, 7)\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        # PyTorch input: (N, C, W, H, D)\n        # JAX input: (N, W, H, D, C)\n        # We will generate data in JAX format.\n        return nn.Conv(features=out_channels, kernel_size=kernel_size, use_bias=False)(x)\n\n# Test code\nbatch_size = 16\nwidth = 64\nheight = 64\ndepth = 64\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, width, height, depth, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)\n", "student_template": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nin_channels = 3\nout_channels = 64\nkernel_size = (3, 5, 7)\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        \"\"\"\n        Complete the conv standard D square input asymmetric kernel forward pass.\n        \"\"\"\n        # TODO: Replace this placeholder with your implementation.\n        # Helpful hints:\n        # - Review the tensors emitted by `get_inputs` so your function signature stays compatible.\n        # - Keep every computation inside JAX/Flax; avoid NumPy on the host to stay TPU-ready.\n        # - Instantiate and call `nn.Conv` with the channel counts shown in the scaffold.\n        # - Reuse the provided PRNG splitting strategy so shapes stay reproducible.\n        raise NotImplementedError('Implement the conv standard D square input asymmetric kernel logic')\n\n# Test code\nbatch_size = 16\nwidth = 64\nheight = 64\ndepth = 64\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, width, height, depth, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **conv standard D square input asymmetric kernel** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "112cc768-6951-4141-a93e-8da2385833db", "title": "This code implements  Matmul with transposed B", "difficulty": "easy", "description": "### Challenge Brief\nSimple model that performs a single matrix multiplication (C = A * B) Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\nM = 1024 * 2\nK = 4096 * 2\nN = 2048 * 2\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(A, B):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B)\n    \"\"\"\n    return jnp.matmul(A, B.T)\n\nM = 1024 * 2\nK = 4096 * 2\nN = 2048 * 2\n\ndef get_inputs(key):\n    \"\"\"\n    Generates random input matrices A and B.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the two input matrices [A, B].\n    \"\"\"\n    key_a, key_b = jax.random.split(key)\n    A = jax.random.normal(key_a, (M, K))\n    B = jax.random.normal(key_b, (N, K))\n    return [A, B]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []\n", "student_template": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(A, B):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B)\n    \"\"\"\n    # TODO: Replace this placeholder with your implementation.\n    # Helpful hints:\n    # - Review the tensors emitted by `get_inputs` so your function signature stays compatible.\n    # - Keep every computation inside JAX/Flax; avoid NumPy on the host to stay TPU-ready.\n    # - Use `jnp.matmul` (or the `@` operator) to perform the core multiply without leaving JAX.\n    # - Pay attention to the explicit transpose operations (e.g., `A.T`, `B.T`) before multiplying.\n    # - Reuse the provided PRNG splitting strategy so shapes stay reproducible.\n    raise NotImplementedError('Implement the Matmul with transposed B logic')\n\nM = 1024 * 2\nK = 4096 * 2\nN = 2048 * 2\n\ndef get_inputs(key):\n    \"\"\"\n    Generates random input matrices A and B.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the two input matrices [A, B].\n    \"\"\"\n    key_a, key_b = jax.random.split(key)\n    A = jax.random.normal(key_a, (M, K))\n    B = jax.random.normal(key_b, (N, K))\n    return [A, B]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **Matmul with transposed B** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "8cf2577e-bca9-4ee9-aa22-52e1b3a7c0df", "title": "This code implements  Matmul with large K dimension", "difficulty": "easy", "description": "### Challenge Brief\nSimple model that performs a single matrix multiplication (C = A * B) with a large K dimension Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\nM = 256\nN = 256\nK = 131072 * 4\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(A, B):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B) with a large K dimension\n    \"\"\"\n    return jnp.matmul(A, B)\n\nM = 256\nN = 256\nK = 131072 * 4\n\ndef get_inputs(key):\n    \"\"\"\n    Generates random input matrices A and B.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the two input matrices [A, B].\n    \"\"\"\n    key_a, key_b = jax.random.split(key)\n    A = jax.random.normal(key_a, (M, K))\n    B = jax.random.normal(key_b, (K, N))\n    return [A, B]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []\n", "student_template": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(A, B):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B) with a large K dimension\n    \"\"\"\n    # TODO: Replace this placeholder with your implementation.\n    # Helpful hints:\n    # - Review the tensors emitted by `get_inputs` so your function signature stays compatible.\n    # - Keep every computation inside JAX/Flax; avoid NumPy on the host to stay TPU-ready.\n    # - Use `jnp.matmul` (or the `@` operator) to perform the core multiply without leaving JAX.\n    # - Reuse the provided PRNG splitting strategy so shapes stay reproducible.\n    raise NotImplementedError('Implement the Matmul with large K dimension logic')\n\nM = 256\nN = 256\nK = 131072 * 4\n\ndef get_inputs(key):\n    \"\"\"\n    Generates random input matrices A and B.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the two input matrices [A, B].\n    \"\"\"\n    key_a, key_b = jax.random.split(key)\n    A = jax.random.normal(key_a, (M, K))\n    B = jax.random.normal(key_b, (K, N))\n    return [A, B]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **Matmul with large K dimension** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "ffb3992e-e907-44e0-8fc5-431e457a8bbc", "title": "This code implements  Square matrix multiplication", "difficulty": "easy", "description": "### Challenge Brief\nSimple model that performs a single square matrix multiplication (C = A * B) Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\nN = 2048 * 2\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(A, B):\n    \"\"\"\n    Simple model that performs a single square matrix multiplication (C = A * B)\n    \"\"\"\n    return jnp.matmul(A, B)\n\nN = 2048 * 2\n\ndef get_inputs(key):\n    \"\"\"\n    Generates random input matrices A and B.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the two input matrices [A, B].\n    \"\"\"\n    key_a, key_b = jax.random.split(key)\n    A = jax.random.normal(key_a, (N, N))\n    B = jax.random.normal(key_b, (N, N))\n    return [A, B]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []\n", "student_template": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(A, B):\n    \"\"\"\n    Simple model that performs a single square matrix multiplication (C = A * B)\n    \"\"\"\n    # TODO: Replace this placeholder with your implementation.\n    # Helpful hints:\n    # - Review the tensors emitted by `get_inputs` so your function signature stays compatible.\n    # - Keep every computation inside JAX/Flax; avoid NumPy on the host to stay TPU-ready.\n    # - Use `jnp.matmul` (or the `@` operator) to perform the core multiply without leaving JAX.\n    # - Reuse the provided PRNG splitting strategy so shapes stay reproducible.\n    raise NotImplementedError('Implement the Square matrix multiplication logic')\n\nN = 2048 * 2\n\ndef get_inputs(key):\n    \"\"\"\n    Generates random input matrices A and B.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the two input matrices [A, B].\n    \"\"\"\n    key_a, key_b = jax.random.split(key)\n    A = jax.random.normal(key_a, (N, N))\n    B = jax.random.normal(key_b, (N, N))\n    return [A, B]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **Square matrix multiplication** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "1ab74bb3-7891-47b2-88a8-9e3b1b1284a7", "title": "This code implements  D tensor matrix multiplication", "difficulty": "easy", "description": "### Challenge Brief\nPerforms 3D tensor-matrix multiplication. Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\nN = 16\nM = 1024\nK = 2048\nL = 768\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(A, B):\n    \"\"\"\n    Performs 3D tensor-matrix multiplication.\n    \"\"\"\n    return jnp.matmul(A, B)\n\nN = 16\nM = 1024\nK = 2048\nL = 768\n\ndef get_inputs(key):\n    \"\"\"\n    Generates random input 3D tensor and matrix.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the input tensor and matrix [A, B].\n    \"\"\"\n    key_a, key_b = jax.random.split(key)\n    A = jax.random.normal(key_a, (N, M, K))\n    B = jax.random.normal(key_b, (K, L))\n    return [A, B]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []\n", "student_template": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(A, B):\n    \"\"\"\n    Performs 3D tensor-matrix multiplication.\n    \"\"\"\n    # TODO: Replace this placeholder with your implementation.\n    # Helpful hints:\n    # - Review the tensors emitted by `get_inputs` so your function signature stays compatible.\n    # - Keep every computation inside JAX/Flax; avoid NumPy on the host to stay TPU-ready.\n    # - Use `jnp.matmul` (or the `@` operator) to perform the core multiply without leaving JAX.\n    # - Reuse the provided PRNG splitting strategy so shapes stay reproducible.\n    raise NotImplementedError('Implement the D tensor matrix multiplication logic')\n\nN = 16\nM = 1024\nK = 2048\nL = 768\n\ndef get_inputs(key):\n    \"\"\"\n    Generates random input 3D tensor and matrix.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the input tensor and matrix [A, B].\n    \"\"\"\n    key_a, key_b = jax.random.split(key)\n    A = jax.random.normal(key_a, (N, M, K))\n    B = jax.random.normal(key_b, (K, L))\n    return [A, B]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **D tensor matrix multiplication** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "bfd3b87a-94af-446a-a7d6-f1d8d5a147bd", "title": "This code implements  Tall skinny matrix multiplication", "difficulty": "easy", "description": "### Challenge Brief\nSimple model that performs a single matrix multiplication (C = A * B) where one of the matrices is tall and skinny (M >> N or N >> M) Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\nM = 16384 * 2\nN = 16 * 2\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(A, B):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B) where one of the matrices is tall and skinny (M >> N or N >> M)\n    \"\"\"\n    return jnp.matmul(A, B)\n\nM = 16384 * 2\nN = 16 * 2\n\ndef get_inputs(key):\n    \"\"\"\n    Generates random input matrices A and B.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the two input matrices [A, B].\n    \"\"\"\n    key_a, key_b = jax.random.split(key)\n    A = jax.random.normal(key_a, (M, N))\n    B = jax.random.normal(key_b, (N, M))\n    return [A, B]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []\n", "student_template": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(A, B):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B) where one of the matrices is tall and skinny (M >> N or N >> M)\n    \"\"\"\n    # TODO: Replace this placeholder with your implementation.\n    # Helpful hints:\n    # - Review the tensors emitted by `get_inputs` so your function signature stays compatible.\n    # - Keep every computation inside JAX/Flax; avoid NumPy on the host to stay TPU-ready.\n    # - Use `jnp.matmul` (or the `@` operator) to perform the core multiply without leaving JAX.\n    # - Reuse the provided PRNG splitting strategy so shapes stay reproducible.\n    raise NotImplementedError('Implement the Tall skinny matrix multiplication logic')\n\nM = 16384 * 2\nN = 16 * 2\n\ndef get_inputs(key):\n    \"\"\"\n    Generates random input matrices A and B.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the two input matrices [A, B].\n    \"\"\"\n    key_a, key_b = jax.random.split(key)\n    A = jax.random.normal(key_a, (M, N))\n    B = jax.random.normal(key_b, (N, M))\n    return [A, B]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **Tall skinny matrix multiplication** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "f7995a20-d227-4228-accc-2d77dc288b7a", "title": "This code implements  Matmul with diagonal matrices", "difficulty": "easy", "description": "### Challenge Brief\nSimple model that performs a matrix multiplication of a diagonal matrix with another matrix. C = diag(A) * B Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\nM = 4096\nN = 4096\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(A, B):\n    \"\"\"\n    Simple model that performs a matrix multiplication of a diagonal matrix with another matrix.\n    C = diag(A) * B\n    \"\"\"\n    return jnp.diag(A) @ B\n\nM = 4096\nN = 4096\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random 1D tensor and a 2D tensor.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the two input tensors [A, B].\n    \"\"\"\n    key_a, key_b = jax.random.split(key)\n    A = jax.random.normal(key_a, (N,))\n    B = jax.random.normal(key_b, (N, M))\n    return [A, B]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []\n", "student_template": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(A, B):\n    \"\"\"\n    Simple model that performs a matrix multiplication of a diagonal matrix with another matrix.\n    C = diag(A) * B\n    \"\"\"\n    # TODO: Replace this placeholder with your implementation.\n    # Helpful hints:\n    # - Review the tensors emitted by `get_inputs` so your function signature stays compatible.\n    # - Keep every computation inside JAX/Flax; avoid NumPy on the host to stay TPU-ready.\n    # - Build or extract diagonals with `jnp.diag` instead of manual indexing.\n    # - Reuse the provided PRNG splitting strategy so shapes stay reproducible.\n    raise NotImplementedError('Implement the Matmul with diagonal matrices logic')\n\nM = 4096\nN = 4096\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random 1D tensor and a 2D tensor.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the two input tensors [A, B].\n    \"\"\"\n    key_a, key_b = jax.random.split(key)\n    A = jax.random.normal(key_a, (N,))\n    B = jax.random.normal(key_b, (N, M))\n    return [A, B]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **Matmul with diagonal matrices** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "6c52b4bf-c5bb-463e-b28a-a23f2084ddeb", "title": "This code implements  Batched matrix multiplication", "difficulty": "easy", "description": "### Challenge Brief\nPerforms batched matrix multiplication (C = A * B) where A, B, and C have the same batch dimension. Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\nbatch_size = 128\nm = 128 * 4\nk = 256 * 4\nn = 512 * 4\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(A, B):\n    \"\"\"\n    Performs batched matrix multiplication (C = A * B) where A, B, and C have the same batch dimension.\n    \"\"\"\n    return jnp.einsum('bij,bjk->bik', A, B)\n\nbatch_size = 128\nm = 128 * 4\nk = 256 * 4\nn = 512 * 4\n\ndef get_inputs(key):\n    \"\"\"\n    Generates random input matrices A and B.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the two input matrices [A, B].\n    \"\"\"\n    key_a, key_b = jax.random.split(key)\n    A = jax.random.normal(key_a, (batch_size, m, k))\n    B = jax.random.normal(key_b, (batch_size, k, n))\n    return [A, B]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []\n", "student_template": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(A, B):\n    \"\"\"\n    Performs batched matrix multiplication (C = A * B) where A, B, and C have the same batch dimension.\n    \"\"\"\n    # TODO: Replace this placeholder with your implementation.\n    # Helpful hints:\n    # - Review the tensors emitted by `get_inputs` so your function signature stays compatible.\n    # - Keep every computation inside JAX/Flax; avoid NumPy on the host to stay TPU-ready.\n    # - Capture the contraction explicitly with `jnp.einsum` so shapes stay readable.\n    # - Reuse the provided PRNG splitting strategy so shapes stay reproducible.\n    raise NotImplementedError('Implement the Batched matrix multiplication logic')\n\nbatch_size = 128\nm = 128 * 4\nk = 256 * 4\nn = 512 * 4\n\ndef get_inputs(key):\n    \"\"\"\n    Generates random input matrices A and B.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the two input matrices [A, B].\n    \"\"\"\n    key_a, key_b = jax.random.split(key)\n    A = jax.random.normal(key_a, (batch_size, m, k))\n    B = jax.random.normal(key_b, (batch_size, k, n))\n    return [A, B]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **Batched matrix multiplication** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "3a7ddaec-ebc1-4ec4-9c43-79a522721f42", "title": "This code implements  conv transposed D asymmetric input square kernel   padded    strided    dilated", "difficulty": "hard", "description": "### Challenge Brief\nFinish implementing **conv transposed D asymmetric input square kernel padded strided dilated** on TPU hardware. Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\nin_channels = 32\nout_channels = 64\nkernel_size = 3\nstride = 2\npadding = 1\ndilation = 2\nbatch_size = 16\nlength = 131072\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nin_channels = 32\nout_channels = 64\nkernel_size = 3\nstride = 2\npadding = 1\ndilation = 2\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        # PyTorch input: (N, C, L)\n        # JAX input: (N, L, C)\n        # We will generate data in JAX format.\n        return nn.ConvTranspose(features=out_channels,\n                                kernel_size=(kernel_size,),\n                                strides=(stride,),\n                                padding=[(padding, padding)],\n                                kernel_dilation=(dilation,),\n                                use_bias=False)(x)\n\n# Test code\nbatch_size = 16\nlength = 131072\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, length, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)\n", "student_template": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nin_channels = 32\nout_channels = 64\nkernel_size = 3\nstride = 2\npadding = 1\ndilation = 2\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        \"\"\"\n        Complete the conv transposed D asymmetric input square kernel padded strided dilated forward pass.\n        \"\"\"\n        # TODO: Replace this placeholder with your implementation.\n        # Helpful hints:\n        # - Review the tensors emitted by `get_inputs` so your function signature stays compatible.\n        # - Keep every computation inside JAX/Flax; avoid NumPy on the host to stay TPU-ready.\n        # - Instantiate and call `nn.Conv` with the channel counts shown in the scaffold.\n        # - Instantiate `nn.ConvTranspose` with the provided strides, kernel size, and channel config.\n        # - Reuse the provided PRNG splitting strategy so shapes stay reproducible.\n        raise NotImplementedError('Implement the conv transposed D asymmetric input square kernel padded strided dilated logic')\n\n# Test code\nbatch_size = 16\nlength = 131072\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, length, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **conv transposed D asymmetric input square kernel padded strided dilated** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "b8aa0cb9-28f0-447b-a549-221f01af3ac2", "title": "This code implements  GELU", "difficulty": "easy", "description": "### Challenge Brief\nSimple model that performs a GELU activation. Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\nbatch_size = 4096\ndim = 393216\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(x):\n    \"\"\"\n    Simple model that performs a GELU activation.\n    \"\"\"\n    return nn.gelu(x)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the input tensor [x].\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []\n", "student_template": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(x):\n    \"\"\"\n    Simple model that performs a GELU activation.\n    \"\"\"\n    # TODO: Replace this placeholder with your implementation.\n    # Helpful hints:\n    # - Review the tensors emitted by `get_inputs` so your function signature stays compatible.\n    # - Keep every computation inside JAX/Flax; avoid NumPy on the host to stay TPU-ready.\n    # - Apply `nn.gelu` (exact or approximate) before projecting.\n    # - Reuse the provided PRNG splitting strategy so shapes stay reproducible.\n    raise NotImplementedError('Implement the GELU logic')\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the input tensor [x].\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **GELU** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "00f6f3cc-b971-402d-847a-6b9cde242fe4", "title": "This code implements  conv transposed D  asymmetric input  square kernel", "difficulty": "hard", "description": "### Challenge Brief\nFinish implementing **conv transposed D asymmetric input square kernel** on TPU hardware. Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\nin_channels = 32\nout_channels = 32\nkernel_size = 3\nbatch_size = 8\nheight_in = 512\nwidth_in = 1024\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nin_channels = 32\nout_channels = 32\nkernel_size = 3\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        # PyTorch input: (N, C, H, W)\n        # JAX input: (N, H, W, C)\n        # We will generate data in JAX format.\n        return nn.ConvTranspose(features=out_channels, kernel_size=(kernel_size, kernel_size), use_bias=False)(x)\n\n# Test code\nbatch_size = 8\nheight_in = 512\nwidth_in = 1024\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, height_in, width_in, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)\n", "student_template": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nin_channels = 32\nout_channels = 32\nkernel_size = 3\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        \"\"\"\n        Complete the conv transposed D asymmetric input square kernel forward pass.\n        \"\"\"\n        # TODO: Replace this placeholder with your implementation.\n        # Helpful hints:\n        # - Review the tensors emitted by `get_inputs` so your function signature stays compatible.\n        # - Keep every computation inside JAX/Flax; avoid NumPy on the host to stay TPU-ready.\n        # - Instantiate and call `nn.Conv` with the channel counts shown in the scaffold.\n        # - Instantiate `nn.ConvTranspose` with the provided strides, kernel size, and channel config.\n        # - Reuse the provided PRNG splitting strategy so shapes stay reproducible.\n        raise NotImplementedError('Implement the conv transposed D asymmetric input square kernel logic')\n\n# Test code\nbatch_size = 8\nheight_in = 512\nwidth_in = 1024\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, height_in, width_in, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **conv transposed D asymmetric input square kernel** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "e311018e-dc79-496e-bcc1-4afd3625cde1", "title": "This code implements  Argmax over a dimension", "difficulty": "easy", "description": "### Challenge Brief\nSimple model that performs Argmax over a specified dimension. Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\nreduce_dim = 1\nbatch_size = 128\ndim1 = 4096\ndim2 = 4095\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nreduce_dim = 1\n\ndef model(x):\n    \"\"\"\n    Simple model that performs Argmax over a specified dimension.\n    \"\"\"\n    return jnp.argmax(x, axis=reduce_dim)\n\nbatch_size = 128\ndim1 = 4096\ndim2 = 4095\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim1, dim2))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n    \"\"\"\n    return []\n", "student_template": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nreduce_dim = 1\n\ndef model(x):\n    \"\"\"\n    Simple model that performs Argmax over a specified dimension.\n    \"\"\"\n    # TODO: Replace this placeholder with your implementation.\n    # Helpful hints:\n    # - Review the tensors emitted by `get_inputs` so your function signature stays compatible.\n    # - Keep every computation inside JAX/Flax; avoid NumPy on the host to stay TPU-ready.\n    # - Remember that `jnp.argmax` needs an `axis` when working on tensors.\n    # - Reuse the provided PRNG splitting strategy so shapes stay reproducible.\n    raise NotImplementedError('Implement the Argmax over a dimension logic')\n\nbatch_size = 128\ndim1 = 4096\ndim2 = 4095\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim1, dim2))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n    \"\"\"\n    return []", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **Argmax over a dimension** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "23ccfdf1-9e1c-48c1-bef9-8eba489dc53f", "title": "This code implements  Softsign", "difficulty": "easy", "description": "### Challenge Brief\nSimple model that performs a Softsign activation. Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\nbatch_size = 4096\ndim = 393216\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(x):\n    \"\"\"\n    Simple model that performs a Softsign activation.\n    \"\"\"\n    return nn.soft_sign(x)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the input tensor [x].\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []\n", "student_template": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(x):\n    \"\"\"\n    Simple model that performs a Softsign activation.\n    \"\"\"\n    # TODO: Replace this placeholder with your implementation.\n    # Helpful hints:\n    # - Review the tensors emitted by `get_inputs` so your function signature stays compatible.\n    # - Keep every computation inside JAX/Flax; avoid NumPy on the host to stay TPU-ready.\n    # - Use `nn.soft_sign` for the indicated normalization.\n    # - Reuse the provided PRNG splitting strategy so shapes stay reproducible.\n    raise NotImplementedError('Implement the Softsign logic')\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the input tensor [x].\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **Softsign** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "6adba5c1-682e-4038-a994-bd7a8e06773e", "title": "This code implements  Tanh", "difficulty": "easy", "description": "### Challenge Brief\nSimple model that performs a Tanh activation. Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\nbatch_size = 4096\ndim = 393216\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(x):\n    \"\"\"\n    Simple model that performs a Tanh activation.\n    \"\"\"\n    return nn.tanh(x)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the input tensor [x].\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []\n", "student_template": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(x):\n    \"\"\"\n    Simple model that performs a Tanh activation.\n    \"\"\"\n    # TODO: Replace this placeholder with your implementation.\n    # Helpful hints:\n    # - Review the tensors emitted by `get_inputs` so your function signature stays compatible.\n    # - Keep every computation inside JAX/Flax; avoid NumPy on the host to stay TPU-ready.\n    # - Use `nn.tanh` to bound activations when the scaffold suggests it.\n    # - Reuse the provided PRNG splitting strategy so shapes stay reproducible.\n    raise NotImplementedError('Implement the Tanh logic')\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the input tensor [x].\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **Tanh** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "999d6a85-b29e-441b-bf5a-b9df71502a7e", "title": "This code implements  ELU", "difficulty": "easy", "description": "### Challenge Brief\nSimple model that performs an ELU activation. Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\nbatch_size = 4096\ndim = 393216\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(x):\n    \"\"\"\n    Simple model that performs an ELU activation.\n    \"\"\"\n    return nn.elu(x, alpha=1.0)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the input tensor [x].\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []\n", "student_template": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(x):\n    \"\"\"\n    Simple model that performs an ELU activation.\n    \"\"\"\n    # TODO: Replace this placeholder with your implementation.\n    # Helpful hints:\n    # - Review the tensors emitted by `get_inputs` so your function signature stays compatible.\n    # - Keep every computation inside JAX/Flax; avoid NumPy on the host to stay TPU-ready.\n    # - Apply `nn.elu` to keep activations bounded below.\n    # - Reuse the provided PRNG splitting strategy so shapes stay reproducible.\n    raise NotImplementedError('Implement the ELU logic')\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the input tensor [x].\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **ELU** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "9c1407ed-fd38-4473-bae5-e1bc09cac196", "title": "This code implements  SELU", "difficulty": "easy", "description": "### Challenge Brief\nSimple model that performs a SELU activation. Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\nbatch_size = 4096\ndim = 393216\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(x):\n    \"\"\"\n    Simple model that performs a SELU activation.\n    \"\"\"\n    return nn.selu(x)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the input tensor [x].\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []\n", "student_template": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(x):\n    \"\"\"\n    Simple model that performs a SELU activation.\n    \"\"\"\n    # TODO: Replace this placeholder with your implementation.\n    # Helpful hints:\n    # - Review the tensors emitted by `get_inputs` so your function signature stays compatible.\n    # - Keep every computation inside JAX/Flax; avoid NumPy on the host to stay TPU-ready.\n    # - SELUs expect properly scaled inputs; use the helper to keep things stable.\n    # - Reuse the provided PRNG splitting strategy so shapes stay reproducible.\n    raise NotImplementedError('Implement the SELU logic')\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the input tensor [x].\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **SELU** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "92580ee7-e09a-4922-b1bb-8c63f64f888e", "title": "This code implements  conv transposed D  square input  square kernel", "difficulty": "hard", "description": "### Challenge Brief\nFinish implementing **conv transposed D square input square kernel** on TPU hardware. Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\nin_channels = 64\nout_channels = 64\nkernel_size = 3\nbatch_size = 8\nheight = 1024\nwidth = 1024\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nin_channels = 64\nout_channels = 64\nkernel_size = 3\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        # PyTorch input: (N, C, H, W)\n        # JAX input: (N, H, W, C)\n        # We will generate data in JAX format.\n        return nn.ConvTranspose(features=out_channels, kernel_size=(kernel_size, kernel_size), use_bias=False)(x)\n\n# Test code\nbatch_size = 8\nheight = 1024\nwidth = 1024\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, height, width, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)\n", "student_template": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nin_channels = 64\nout_channels = 64\nkernel_size = 3\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        \"\"\"\n        Complete the conv transposed D square input square kernel forward pass.\n        \"\"\"\n        # TODO: Replace this placeholder with your implementation.\n        # Helpful hints:\n        # - Review the tensors emitted by `get_inputs` so your function signature stays compatible.\n        # - Keep every computation inside JAX/Flax; avoid NumPy on the host to stay TPU-ready.\n        # - Instantiate and call `nn.Conv` with the channel counts shown in the scaffold.\n        # - Instantiate `nn.ConvTranspose` with the provided strides, kernel size, and channel config.\n        # - Reuse the provided PRNG splitting strategy so shapes stay reproducible.\n        raise NotImplementedError('Implement the conv transposed D square input square kernel logic')\n\n# Test code\nbatch_size = 8\nheight = 1024\nwidth = 1024\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, height, width, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **conv transposed D square input square kernel** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "142b5c18-5369-4ce2-90cb-19889bc506aa", "title": "This code implements  Swish", "difficulty": "easy", "description": "### Challenge Brief\nSimple model that performs a Swish activation. Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\nbatch_size = 4096\ndim = 393216\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(x):\n    \"\"\"\n    Simple model that performs a Swish activation.\n    \"\"\"\n    return nn.swish(x)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the input tensor [x].\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []\n", "student_template": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(x):\n    \"\"\"\n    Simple model that performs a Swish activation.\n    \"\"\"\n    # TODO: Replace this placeholder with your implementation.\n    # Helpful hints:\n    # - Review the tensors emitted by `get_inputs` so your function signature stays compatible.\n    # - Keep every computation inside JAX/Flax; avoid NumPy on the host to stay TPU-ready.\n    # - Use `nn.swish` for smooth activations before the projection.\n    # - Reuse the provided PRNG splitting strategy so shapes stay reproducible.\n    raise NotImplementedError('Implement the Swish logic')\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the input tensor [x].\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **Swish** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "4b9e94bc-7c68-4608-8cdf-2e6eb769db6f", "title": "This code implements  conv transposed D  asymmetric input  asymmetric kernel", "difficulty": "hard", "description": "### Challenge Brief\nFinish implementing **conv transposed D asymmetric input asymmetric kernel** on TPU hardware. Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\nin_channels = 32\nout_channels = 16\nkernel_size = (3, 5, 7)\nbatch_size = 16\ndepth_in = 16\nheight_in = 32\nwidth_in = 64\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nin_channels = 32\nout_channels = 16\nkernel_size = (3, 5, 7)\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        # PyTorch input: (N, C, D, H, W)\n        # JAX input: (N, D, H, W, C)\n        # We will generate data in JAX format.\n        return nn.ConvTranspose(features=out_channels, kernel_size=kernel_size, use_bias=False)(x)\n\n# Test code\nbatch_size = 16\ndepth_in = 16\nheight_in = 32\nwidth_in = 64\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, depth_in, height_in, width_in, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)\n", "student_template": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nin_channels = 32\nout_channels = 16\nkernel_size = (3, 5, 7)\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        \"\"\"\n        Complete the conv transposed D asymmetric input asymmetric kernel forward pass.\n        \"\"\"\n        # TODO: Replace this placeholder with your implementation.\n        # Helpful hints:\n        # - Review the tensors emitted by `get_inputs` so your function signature stays compatible.\n        # - Keep every computation inside JAX/Flax; avoid NumPy on the host to stay TPU-ready.\n        # - Instantiate and call `nn.Conv` with the channel counts shown in the scaffold.\n        # - Instantiate `nn.ConvTranspose` with the provided strides, kernel size, and channel config.\n        # - Reuse the provided PRNG splitting strategy so shapes stay reproducible.\n        raise NotImplementedError('Implement the conv transposed D asymmetric input asymmetric kernel logic')\n\n# Test code\nbatch_size = 16\ndepth_in = 16\nheight_in = 32\nwidth_in = 64\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, depth_in, height_in, width_in, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **conv transposed D asymmetric input asymmetric kernel** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "ca79a6c2-5e8d-411d-b867-a48e952149a3", "title": "This code implements  conv transposed D square input square kernel   padded    dilated    strided", "difficulty": "hard", "description": "### Challenge Brief\nFinish implementing **conv transposed D square input square kernel padded dilated strided** on TPU hardware. Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\nin_channels = 32\nout_channels = 64\nkernel_size = 3\nstride = 2\npadding = 1\ndilation = 2\nbatch_size = 16\ndepth = 16\nheight = 32\nwidth = 32\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nin_channels = 32\nout_channels = 64\nkernel_size = 3\nstride = 2\npadding = 1\ndilation = 2\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        # PyTorch input: (N, C, D, H, W)\n        # JAX input: (N, D, H, W, C)\n        # We will generate data in JAX format.\n        return nn.ConvTranspose(features=out_channels,\n                                kernel_size=(kernel_size, kernel_size, kernel_size),\n                                strides=(stride, stride, stride),\n                                padding=[(padding, padding), (padding, padding), (padding, padding)],\n                                kernel_dilation=(dilation, dilation, dilation),\n                                use_bias=False)(x)\n\n# Test code\nbatch_size = 16\ndepth = 16\nheight = 32\nwidth = 32\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, depth, height, width, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)\n", "student_template": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nin_channels = 32\nout_channels = 64\nkernel_size = 3\nstride = 2\npadding = 1\ndilation = 2\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        \"\"\"\n        Complete the conv transposed D square input square kernel padded dilated strided forward pass.\n        \"\"\"\n        # TODO: Replace this placeholder with your implementation.\n        # Helpful hints:\n        # - Review the tensors emitted by `get_inputs` so your function signature stays compatible.\n        # - Keep every computation inside JAX/Flax; avoid NumPy on the host to stay TPU-ready.\n        # - Instantiate and call `nn.Conv` with the channel counts shown in the scaffold.\n        # - Instantiate `nn.ConvTranspose` with the provided strides, kernel size, and channel config.\n        # - Reuse the provided PRNG splitting strategy so shapes stay reproducible.\n        raise NotImplementedError('Implement the conv transposed D square input square kernel padded dilated strided logic')\n\n# Test code\nbatch_size = 16\ndepth = 16\nheight = 32\nwidth = 32\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, depth, height, width, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **conv transposed D square input square kernel padded dilated strided** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "21ad986b-68ff-462a-ab6c-6a35aa8c0652", "title": "This code implements  conv standard D  asymmetric input  asymmetric kernel", "difficulty": "hard", "description": "### Challenge Brief\nFinish implementing **conv standard D asymmetric input asymmetric kernel** on TPU hardware. Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\nin_channels = 3\nout_channels = 64\nkernel_size = (3, 5, 7)\nbatch_size = 8\ndepth = 16\nheight = 128\nwidth = 128\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nin_channels = 3\nout_channels = 64\nkernel_size = (3, 5, 7)\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        # PyTorch input: (N, C, D, H, W)\n        # JAX input: (N, D, H, W, C)\n        # We will generate data in JAX format.\n        return nn.Conv(features=out_channels, kernel_size=kernel_size, use_bias=False)(x)\n\n# Test code\nbatch_size = 8\ndepth = 16\nheight = 128\nwidth = 128\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, depth, height, width, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)\n", "student_template": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nin_channels = 3\nout_channels = 64\nkernel_size = (3, 5, 7)\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        \"\"\"\n        Complete the conv standard D asymmetric input asymmetric kernel forward pass.\n        \"\"\"\n        # TODO: Replace this placeholder with your implementation.\n        # Helpful hints:\n        # - Review the tensors emitted by `get_inputs` so your function signature stays compatible.\n        # - Keep every computation inside JAX/Flax; avoid NumPy on the host to stay TPU-ready.\n        # - Instantiate and call `nn.Conv` with the channel counts shown in the scaffold.\n        # - Reuse the provided PRNG splitting strategy so shapes stay reproducible.\n        raise NotImplementedError('Implement the conv standard D asymmetric input asymmetric kernel logic')\n\n# Test code\nbatch_size = 8\ndepth = 16\nheight = 128\nwidth = 128\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, depth, height, width, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **conv standard D asymmetric input asymmetric kernel** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "3cadd613-1236-47f9-a3d8-8f434e0398d5", "title": "This code implements  conv transposed D", "difficulty": "hard", "description": "### Challenge Brief\nFinish implementing **conv transposed D** on TPU hardware. Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\nin_channels = 128\nout_channels = 128\nkernel_size = 3\nbatch_size = 64\nlength = 65536\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nin_channels = 128\nout_channels = 128\nkernel_size = 3\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        # PyTorch input: (N, C, L)\n        # JAX input: (N, L, C)\n        # We will generate data in JAX format.\n        return nn.ConvTranspose(features=out_channels, kernel_size=(kernel_size,), use_bias=False)(x)\n\n# Test code\nbatch_size = 64\nlength = 65536\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, length, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)\n", "student_template": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nin_channels = 128\nout_channels = 128\nkernel_size = 3\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        \"\"\"\n        Complete the conv transposed D forward pass.\n        \"\"\"\n        # TODO: Replace this placeholder with your implementation.\n        # Helpful hints:\n        # - Review the tensors emitted by `get_inputs` so your function signature stays compatible.\n        # - Keep every computation inside JAX/Flax; avoid NumPy on the host to stay TPU-ready.\n        # - Instantiate and call `nn.Conv` with the channel counts shown in the scaffold.\n        # - Instantiate `nn.ConvTranspose` with the provided strides, kernel size, and channel config.\n        # - Reuse the provided PRNG splitting strategy so shapes stay reproducible.\n        raise NotImplementedError('Implement the conv transposed D logic')\n\n# Test code\nbatch_size = 64\nlength = 65536\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, length, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **conv transposed D** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "8acfd3d3-be83-4a9c-bd2d-eab4de8c9969", "title": "This code implements  conv standard D dilated strided", "difficulty": "hard", "description": "### Challenge Brief\nFinish implementing **conv standard D dilated strided** on TPU hardware. Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\nin_channels = 64\nout_channels = 128\nkernel_size = 3\nstride = 3\ndilation = 4\nbatch_size = 64\nlength = 524280\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nin_channels = 64\nout_channels = 128\nkernel_size = 3\nstride = 3\ndilation = 4\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        # PyTorch input: (N, C, L)\n        # JAX input: (N, L, C)\n        # We will generate data in JAX format.\n        return nn.Conv(features=out_channels,\n                       kernel_size=(kernel_size,),\n                       strides=(stride,),\n                       kernel_dilation=(dilation,),\n                       use_bias=False)(x)\n\n# Test code\nbatch_size = 64\nlength = 524280\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, length, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)\n", "student_template": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nin_channels = 64\nout_channels = 128\nkernel_size = 3\nstride = 3\ndilation = 4\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        \"\"\"\n        Complete the conv standard D dilated strided forward pass.\n        \"\"\"\n        # TODO: Replace this placeholder with your implementation.\n        # Helpful hints:\n        # - Review the tensors emitted by `get_inputs` so your function signature stays compatible.\n        # - Keep every computation inside JAX/Flax; avoid NumPy on the host to stay TPU-ready.\n        # - Instantiate and call `nn.Conv` with the channel counts shown in the scaffold.\n        # - Reuse the provided PRNG splitting strategy so shapes stay reproducible.\n        raise NotImplementedError('Implement the conv standard D dilated strided logic')\n\n# Test code\nbatch_size = 64\nlength = 524280\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, length, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **conv standard D dilated strided** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "3b1c2ed4-7b15-447e-8c46-c3f2e5bbb536", "title": "This code implements  Softplus", "difficulty": "easy", "description": "### Challenge Brief\nSimple model that performs a Softplus activation. Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\nbatch_size = 4096\ndim = 393216\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(x):\n    \"\"\"\n    Simple model that performs a Softplus activation.\n    \"\"\"\n    return nn.softplus(x)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the input tensor [x].\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []\n", "student_template": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(x):\n    \"\"\"\n    Simple model that performs a Softplus activation.\n    \"\"\"\n    # TODO: Replace this placeholder with your implementation.\n    # Helpful hints:\n    # - Review the tensors emitted by `get_inputs` so your function signature stays compatible.\n    # - Keep every computation inside JAX/Flax; avoid NumPy on the host to stay TPU-ready.\n    # - Apply `nn.softplus` for smooth positive activations.\n    # - Reuse the provided PRNG splitting strategy so shapes stay reproducible.\n    raise NotImplementedError('Implement the Softplus logic')\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the input tensor [x].\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **Softplus** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "aacb121b-d739-4fe0-815b-2fb68cb306be", "title": "This code implements  conv transposed D asymmetric input asymmetric kernel   strided padded grouped", "difficulty": "hard", "description": "### Challenge Brief\nFinish implementing **conv transposed D asymmetric input asymmetric kernel strided padded grouped** on TPU hardware. Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\nin_channels = 32\nout_channels = 32\nkernel_size = (3, 5, 7)\nstride = (2, 2, 2)\npadding = (1, 2, 3)\noutput_padding = (1, 1, 1)\ngroups = 4\nbatch_size = 8\ndepth = 12\nheight = 24\nwidth = 48\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nin_channels = 32\nout_channels = 32\nkernel_size = (3, 5, 7)\nstride = (2, 2, 2)\npadding = (1, 2, 3)\noutput_padding = (1, 1, 1)\ngroups = 4\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        # PyTorch input: (N, C, D, H, W)\n        # JAX input: (N, D, H, W, C)\n        # We will generate data in JAX format.\n        \n        # Adjust padding to account for output_padding\n        padding_jax = [\n            (padding[0], padding[0] - output_padding[0]),\n            (padding[1], padding[1] - output_padding[1]),\n            (padding[2], padding[2] - output_padding[2])\n        ]\n        \n        return nn.ConvTranspose(features=out_channels,\n                                kernel_size=kernel_size,\n                                strides=stride,\n                                padding=padding_jax,\n                                feature_group_count=groups,\n                                use_bias=False)(x)\n\n# Test code\nbatch_size = 8\ndepth = 12\nheight = 24\nwidth = 48\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, depth, height, width, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)\n", "student_template": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nin_channels = 32\nout_channels = 32\nkernel_size = (3, 5, 7)\nstride = (2, 2, 2)\npadding = (1, 2, 3)\noutput_padding = (1, 1, 1)\ngroups = 4\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        \"\"\"\n        Complete the conv transposed D asymmetric input asymmetric kernel strided padded grouped forward pass.\n        \"\"\"\n        # TODO: Replace this placeholder with your implementation.\n        # Helpful hints:\n        # - Review the tensors emitted by `get_inputs` so your function signature stays compatible.\n        # - Keep every computation inside JAX/Flax; avoid NumPy on the host to stay TPU-ready.\n        # - Instantiate and call `nn.Conv` with the channel counts shown in the scaffold.\n        # - Instantiate `nn.ConvTranspose` with the provided strides, kernel size, and channel config.\n        # - Reuse the provided PRNG splitting strategy so shapes stay reproducible.\n        raise NotImplementedError('Implement the conv transposed D asymmetric input asymmetric kernel strided padded grouped logic')\n\n# Test code\nbatch_size = 8\ndepth = 12\nheight = 24\nwidth = 48\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, depth, height, width, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **conv transposed D asymmetric input asymmetric kernel strided padded grouped** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "070f54e3-19d8-46a1-a5b1-a5e8b983ae79", "title": "This code implements  conv transposed D dilated", "difficulty": "hard", "description": "### Challenge Brief\nFinish implementing **conv transposed D dilated** on TPU hardware. Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\nin_channels = 32\nout_channels = 64\nkernel_size = 5\nstride = 1\npadding = 0\ndilation = 3\nbatch_size = 32\nlength = 131072\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nin_channels = 32\nout_channels = 64\nkernel_size = 5\nstride = 1\npadding = 0\ndilation = 3\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        # PyTorch input: (N, C, L)\n        # JAX input: (N, L, C)\n        # We will generate data in JAX format.\n        return nn.ConvTranspose(features=out_channels,\n                                kernel_size=(kernel_size,),\n                                strides=(stride,),\n                                padding=[(padding, padding)],\n                                kernel_dilation=(dilation,),\n                                use_bias=False)(x)\n\n# Test code\nbatch_size = 32\nlength = 131072\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, length, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)\n", "student_template": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nin_channels = 32\nout_channels = 64\nkernel_size = 5\nstride = 1\npadding = 0\ndilation = 3\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        \"\"\"\n        Complete the conv transposed D dilated forward pass.\n        \"\"\"\n        # TODO: Replace this placeholder with your implementation.\n        # Helpful hints:\n        # - Review the tensors emitted by `get_inputs` so your function signature stays compatible.\n        # - Keep every computation inside JAX/Flax; avoid NumPy on the host to stay TPU-ready.\n        # - Instantiate and call `nn.Conv` with the channel counts shown in the scaffold.\n        # - Instantiate `nn.ConvTranspose` with the provided strides, kernel size, and channel config.\n        # - Reuse the provided PRNG splitting strategy so shapes stay reproducible.\n        raise NotImplementedError('Implement the conv transposed D dilated logic')\n\n# Test code\nbatch_size = 32\nlength = 131072\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, length, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **conv transposed D dilated** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "8eb0cdca-f49c-4eb6-919e-b05a5ac445e9", "title": "This code implements  conv standard D  asymmetric input  square kernel", "difficulty": "hard", "description": "### Challenge Brief\nFinish implementing **conv standard D asymmetric input square kernel** on TPU hardware. Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\nin_channels = 64\nout_channels = 128\nkernel_size = 3\nbatch_size = 8\nheight = 512\nwidth = 1024\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nin_channels = 64\nout_channels = 128\nkernel_size = 3\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        # PyTorch input: (N, C, H, W)\n        # JAX input: (N, H, W, C)\n        # We will generate data in JAX format.\n        return nn.Conv(features=out_channels, kernel_size=(kernel_size, kernel_size), use_bias=False)(x)\n\n# Test code\nbatch_size = 8\n# smaller spatial dims\nheight = 512\nwidth = 1024\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, height, width, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)\n", "student_template": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nin_channels = 64\nout_channels = 128\nkernel_size = 3\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        \"\"\"\n        Complete the conv standard D asymmetric input square kernel forward pass.\n        \"\"\"\n        # TODO: Replace this placeholder with your implementation.\n        # Helpful hints:\n        # - Review the tensors emitted by `get_inputs` so your function signature stays compatible.\n        # - Keep every computation inside JAX/Flax; avoid NumPy on the host to stay TPU-ready.\n        # - Instantiate and call `nn.Conv` with the channel counts shown in the scaffold.\n        # - Reuse the provided PRNG splitting strategy so shapes stay reproducible.\n        raise NotImplementedError('Implement the conv standard D asymmetric input square kernel logic')\n\n# Test code\nbatch_size = 8\n# smaller spatial dims\nheight = 512\nwidth = 1024\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, height, width, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **conv standard D asymmetric input square kernel** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "407b1a6c-6e0f-4a3e-963f-a89aca4d38a7", "title": "This code implements  conv standard D  square input  square kernel", "difficulty": "hard", "description": "### Challenge Brief\nFinish implementing **conv standard D square input square kernel** on TPU hardware. Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\nbatch_size = 256\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        # PyTorch input: (N, C, H, W)\n        # JAX input: (N, H, W, C)\n        # We will generate data in JAX format.\n        x = nn.Conv(features=96, kernel_size=(11, 11), strides=(4, 4), padding=[(2,2),(2,2)])(x)\n        return x\n\n# Test code\nbatch_size = 256\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, 224, 224, 3))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)\n", "student_template": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        \"\"\"\n        Complete the conv standard D square input square kernel forward pass.\n        \"\"\"\n        # TODO: Replace this placeholder with your implementation.\n        # Helpful hints:\n        # - Review the tensors emitted by `get_inputs` so your function signature stays compatible.\n        # - Keep every computation inside JAX/Flax; avoid NumPy on the host to stay TPU-ready.\n        # - Instantiate and call `nn.Conv` with the channel counts shown in the scaffold.\n        # - Reuse the provided PRNG splitting strategy so shapes stay reproducible.\n        raise NotImplementedError('Implement the conv standard D square input square kernel logic')\n\n# Test code\nbatch_size = 256\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, 224, 224, 3))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **conv standard D square input square kernel** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "c1db9524-d3db-459b-a738-377c1bdea89b", "title": "This code implements  Matmul with irregular shapes", "difficulty": "easy", "description": "### Challenge Brief\nSimple model that performs a single matrix multiplication (C = A * B) with irregular shapes Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\nM = 8205\nK = 2949\nN = 5921\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(A, B):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B) with irregular shapes\n    \"\"\"\n    return jnp.matmul(A, B)\n\nM = 8205\nK = 2949\nN = 5921\n\ndef get_inputs(key):\n    \"\"\"\n    Generates random input matrices A and B.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the two input matrices [A, B].\n    \"\"\"\n    key_a, key_b = jax.random.split(key)\n    A = jax.random.normal(key_a, (M, K))\n    B = jax.random.normal(key_b, (K, N))\n    return [A, B]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []\n", "student_template": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(A, B):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B) with irregular shapes\n    \"\"\"\n    # TODO: Replace this placeholder with your implementation.\n    # Helpful hints:\n    # - Review the tensors emitted by `get_inputs` so your function signature stays compatible.\n    # - Keep every computation inside JAX/Flax; avoid NumPy on the host to stay TPU-ready.\n    # - Use `jnp.matmul` (or the `@` operator) to perform the core multiply without leaving JAX.\n    # - Reuse the provided PRNG splitting strategy so shapes stay reproducible.\n    raise NotImplementedError('Implement the Matmul with irregular shapes logic')\n\nM = 8205\nK = 2949\nN = 5921\n\ndef get_inputs(key):\n    \"\"\"\n    Generates random input matrices A and B.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the two input matrices [A, B].\n    \"\"\"\n    key_a, key_b = jax.random.split(key)\n    A = jax.random.normal(key_a, (M, K))\n    B = jax.random.normal(key_b, (K, N))\n    return [A, B]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **Matmul with irregular shapes** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "d5f10f40-0b09-48a5-8f39-f27ff77eb871", "title": "This code implements  Standard matrix multiplication", "difficulty": "easy", "description": "### Challenge Brief\nSimple model that performs a single matrix multiplication (C = A * B) Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\nM = 1024 * 2\nK = 4096 * 2\nN = 2048 * 2\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(A, B):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B)\n    \"\"\"\n    return jnp.matmul(A, B)\n\nM = 1024 * 2\nK = 4096 * 2\nN = 2048 * 2\n\ndef get_inputs(key):\n    \"\"\"\n    Generates random input matrices A and B.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the two input matrices [A, B].\n    \"\"\"\n    key_a, key_b = jax.random.split(key)\n    A = jax.random.normal(key_a, (M, K))\n    B = jax.random.normal(key_b, (K, N))\n    return [A, B]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []\n", "student_template": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(A, B):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B)\n    \"\"\"\n    # TODO: Replace this placeholder with your implementation.\n    # Helpful hints:\n    # - Review the tensors emitted by `get_inputs` so your function signature stays compatible.\n    # - Keep every computation inside JAX/Flax; avoid NumPy on the host to stay TPU-ready.\n    # - Use `jnp.matmul` (or the `@` operator) to perform the core multiply without leaving JAX.\n    # - Reuse the provided PRNG splitting strategy so shapes stay reproducible.\n    raise NotImplementedError('Implement the Standard matrix multiplication logic')\n\nM = 1024 * 2\nK = 4096 * 2\nN = 2048 * 2\n\ndef get_inputs(key):\n    \"\"\"\n    Generates random input matrices A and B.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the two input matrices [A, B].\n    \"\"\"\n    key_a, key_b = jax.random.split(key)\n    A = jax.random.normal(key_a, (M, K))\n    B = jax.random.normal(key_b, (K, N))\n    return [A, B]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **Standard matrix multiplication** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "e4984384-0573-4e6e-804e-63829111b03b", "title": "This code implements  conv standard D", "difficulty": "hard", "description": "### Challenge Brief\nFinish implementing **conv standard D** on TPU hardware. Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\nin_channels = 64\nout_channels = 128\nkernel_size = 3\nbatch_size = 32\nlength = 131072\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nin_channels = 64\nout_channels = 128\nkernel_size = 3\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        # PyTorch input: (N, C, L)\n        # JAX input: (N, L, C)\n        # We will generate data in JAX format.\n        return nn.Conv(features=out_channels, kernel_size=(kernel_size,), use_bias=False)(x)\n\n# Test code\nbatch_size = 32\nlength = 131072\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, length, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)\n", "student_template": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nin_channels = 64\nout_channels = 128\nkernel_size = 3\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        \"\"\"\n        Complete the conv standard D forward pass.\n        \"\"\"\n        # TODO: Replace this placeholder with your implementation.\n        # Helpful hints:\n        # - Review the tensors emitted by `get_inputs` so your function signature stays compatible.\n        # - Keep every computation inside JAX/Flax; avoid NumPy on the host to stay TPU-ready.\n        # - Instantiate and call `nn.Conv` with the channel counts shown in the scaffold.\n        # - Reuse the provided PRNG splitting strategy so shapes stay reproducible.\n        raise NotImplementedError('Implement the conv standard D logic')\n\n# Test code\nbatch_size = 32\nlength = 131072\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, length, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **conv standard D** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "9ce78824-9003-4485-8160-43126a2266a1", "title": "This code implements  Matmul for symmetric matrices", "difficulty": "easy", "description": "### Challenge Brief\nSimple model that performs a single matrix multiplication (C = A * B) with A and B being symmetric matrices. Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\nN = 4096\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(A, B):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B) with A and B being symmetric matrices.\n    \"\"\"\n    return jnp.matmul(A, B)\n\nN = 4096\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a pair of random symmetric matrices for testing.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: List containing two symmetric tensors A and B.\n    \"\"\"\n    key_a, key_b = jax.random.split(key)\n    A = jax.random.normal(key_a, (N, N))\n    A = (A + A.T) / 2  # Ensure symmetry\n    B = jax.random.normal(key_b, (N, N))\n    B = (B + B.T) / 2  # Ensure symmetry\n    return [A, B]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No specific initialization inputs needed for this model.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: Empty list.\n    \"\"\"\n    return []\n", "student_template": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(A, B):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B) with A and B being symmetric matrices.\n    \"\"\"\n    # TODO: Replace this placeholder with your implementation.\n    # Helpful hints:\n    # - Review the tensors emitted by `get_inputs` so your function signature stays compatible.\n    # - Keep every computation inside JAX/Flax; avoid NumPy on the host to stay TPU-ready.\n    # - Use `jnp.matmul` (or the `@` operator) to perform the core multiply without leaving JAX.\n    # - Pay attention to the explicit transpose operations (e.g., `A.T`, `B.T`) before multiplying.\n    # - Reuse the provided PRNG splitting strategy so shapes stay reproducible.\n    raise NotImplementedError('Implement the Matmul for symmetric matrices logic')\n\nN = 4096\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a pair of random symmetric matrices for testing.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: List containing two symmetric tensors A and B.\n    \"\"\"\n    key_a, key_b = jax.random.split(key)\n    A = jax.random.normal(key_a, (N, N))\n    A = (A + A.T) / 2  # Ensure symmetry\n    B = jax.random.normal(key_b, (N, N))\n    B = (B + B.T) / 2  # Ensure symmetry\n    return [A, B]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No specific initialization inputs needed for this model.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: Empty list.\n    \"\"\"\n    return []", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **Matmul for symmetric matrices** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "d3d5af30-7814-41fc-8989-7d37f5f683c6", "title": "This code implements  Matmul with transposed both", "difficulty": "easy", "description": "### Challenge Brief\nSimple model that performs a single matrix multiplication (C = A * B) Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\nM = 1024 * 2\nK = 4096 * 2\nN = 2048 * 2\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(A, B):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B)\n    \"\"\"\n    return jnp.matmul(A.T, B.T)\n\nM = 1024 * 2\nK = 4096 * 2\nN = 2048 * 2\n\ndef get_inputs(key):\n    \"\"\"\n    Generates random input matrices A and B.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the two input matrices [A, B].\n    \"\"\"\n    key_a, key_b = jax.random.split(key)\n    A = jax.random.normal(key_a, (K, M))\n    B = jax.random.normal(key_b, (N, K))\n    return [A, B]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []\n", "student_template": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(A, B):\n    \"\"\"\n    Simple model that performs a single matrix multiplication (C = A * B)\n    \"\"\"\n    # TODO: Replace this placeholder with your implementation.\n    # Helpful hints:\n    # - Review the tensors emitted by `get_inputs` so your function signature stays compatible.\n    # - Keep every computation inside JAX/Flax; avoid NumPy on the host to stay TPU-ready.\n    # - Use `jnp.matmul` (or the `@` operator) to perform the core multiply without leaving JAX.\n    # - Pay attention to the explicit transpose operations (e.g., `A.T`, `B.T`) before multiplying.\n    # - Reuse the provided PRNG splitting strategy so shapes stay reproducible.\n    raise NotImplementedError('Implement the Matmul with transposed both logic')\n\nM = 1024 * 2\nK = 4096 * 2\nN = 2048 * 2\n\ndef get_inputs(key):\n    \"\"\"\n    Generates random input matrices A and B.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the two input matrices [A, B].\n    \"\"\"\n    key_a, key_b = jax.random.split(key)\n    A = jax.random.normal(key_a, (K, M))\n    B = jax.random.normal(key_b, (N, K))\n    return [A, B]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **Matmul with transposed both** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "b7f6aa88-e2fa-4cac-8991-49089932c56c", "title": "This code implements  conv standard D  asymmetric input  square kernel", "difficulty": "hard", "description": "### Challenge Brief\nFinish implementing **conv standard D asymmetric input square kernel** on TPU hardware. Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\nin_channels = 3\nout_channels = 64\nkernel_size = 3\nbatch_size = 16\nwidth = 256\nheight = 256\ndepth = 10\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nin_channels = 3\nout_channels = 64\nkernel_size = 3\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        # PyTorch input: (N, C, H, W, D)\n        # JAX input: (N, H, W, D, C)\n        # We will generate data in JAX format.\n        return nn.Conv(features=out_channels, kernel_size=(kernel_size, kernel_size, 1), use_bias=False)(x)\n\n# Test code\nbatch_size = 16\nwidth = 256\nheight = 256\ndepth = 10\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, height, width, depth, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)\n", "student_template": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nin_channels = 3\nout_channels = 64\nkernel_size = 3\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        \"\"\"\n        Complete the conv standard D asymmetric input square kernel forward pass.\n        \"\"\"\n        # TODO: Replace this placeholder with your implementation.\n        # Helpful hints:\n        # - Review the tensors emitted by `get_inputs` so your function signature stays compatible.\n        # - Keep every computation inside JAX/Flax; avoid NumPy on the host to stay TPU-ready.\n        # - Instantiate and call `nn.Conv` with the channel counts shown in the scaffold.\n        # - Reuse the provided PRNG splitting strategy so shapes stay reproducible.\n        raise NotImplementedError('Implement the conv standard D asymmetric input square kernel logic')\n\n# Test code\nbatch_size = 16\nwidth = 256\nheight = 256\ndepth = 10\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, height, width, depth, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **conv standard D asymmetric input square kernel** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "e38189f3-b76f-484c-b09d-5d4afbc7e674", "title": "This code implements  ReLU", "difficulty": "easy", "description": "### Challenge Brief\nSimple model that performs a ReLU activation. Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\nbatch_size = 4096\ndim = 393216\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(x):\n    \"\"\"\n    Simple model that performs a ReLU activation.\n    \"\"\"\n    return nn.relu(x)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the input tensor [x].\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []\n", "student_template": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(x):\n    \"\"\"\n    Simple model that performs a ReLU activation.\n    \"\"\"\n    # TODO: Replace this placeholder with your implementation.\n    # Helpful hints:\n    # - Review the tensors emitted by `get_inputs` so your function signature stays compatible.\n    # - Keep every computation inside JAX/Flax; avoid NumPy on the host to stay TPU-ready.\n    # - Keep activations inside TPU by using `nn.relu` (or `jax.nn.relu`).\n    # - Reuse the provided PRNG splitting strategy so shapes stay reproducible.\n    raise NotImplementedError('Implement the ReLU logic')\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the input tensor [x].\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **ReLU** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "2a223848-1ecf-4c03-bf3f-b2090c2b199f", "title": "This code implements  Sigmoid", "difficulty": "easy", "description": "### Challenge Brief\nSimple model that performs a Sigmoid activation. Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\nbatch_size = 4096\ndim = 393216\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(x):\n    \"\"\"\n    Simple model that performs a Sigmoid activation.\n    \"\"\"\n    return nn.sigmoid(x)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the input tensor [x].\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []\n", "student_template": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(x):\n    \"\"\"\n    Simple model that performs a Sigmoid activation.\n    \"\"\"\n    # TODO: Replace this placeholder with your implementation.\n    # Helpful hints:\n    # - Review the tensors emitted by `get_inputs` so your function signature stays compatible.\n    # - Keep every computation inside JAX/Flax; avoid NumPy on the host to stay TPU-ready.\n    # - Use `nn.sigmoid` to squeeze values into [0, 1] where indicated.\n    # - Reuse the provided PRNG splitting strategy so shapes stay reproducible.\n    raise NotImplementedError('Implement the Sigmoid logic')\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the input tensor [x].\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **Sigmoid** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "4f767f11-52c0-4385-9798-e168cb25683c", "title": "This code implements  Average Pooling D", "difficulty": "medium", "description": "### Challenge Brief\nFinish implementing **Average Pooling D** on TPU hardware. Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\nkernel_size = 11\nbatch_size = 16\nchannels = 64\nheight = 2048\nwidth = 2048\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nkernel_size = 11\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        # PyTorch input: (N, C, H, W)\n        # JAX input: (N, H, W, C)\n        # We will generate data in JAX format.\n        return nn.avg_pool(x,\n                           window_shape=(kernel_size, kernel_size),\n                           strides=(kernel_size, kernel_size),\n                           padding='VALID')\n\nbatch_size = 16\nchannels = 64\nheight = 2048\nwidth = 2048\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, height, width, channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)\n", "student_template": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nkernel_size = 11\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        \"\"\"\n        Complete the Average Pooling D forward pass.\n        \"\"\"\n        # TODO: Replace this placeholder with your implementation.\n        # Helpful hints:\n        # - Review the tensors emitted by `get_inputs` so your function signature stays compatible.\n        # - Keep every computation inside JAX/Flax; avoid NumPy on the host to stay TPU-ready.\n        # - Mirror the provided window/stride/padding hyperparameters when calling `nn.avg_pool`.\n        # - Reuse the provided PRNG splitting strategy so shapes stay reproducible.\n        raise NotImplementedError('Implement the Average Pooling D logic')\n\nbatch_size = 16\nchannels = 64\nheight = 2048\nwidth = 2048\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, height, width, channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **Average Pooling D** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "dc1e5c8b-d9d6-4a2b-8aee-2f5c32fde8b6", "title": "This code implements  conv transposed D  square input  square kernel", "difficulty": "hard", "description": "### Challenge Brief\nFinish implementing **conv transposed D square input square kernel** on TPU hardware. Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\nin_channels = 48\nout_channels = 48\nkernel_size = 3\nbatch_size = 8\ndepth = 64\nheight = 64\nwidth = 64\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nin_channels = 48\nout_channels = 48\nkernel_size = 3\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        # PyTorch input: (N, C, D, H, W)\n        # JAX input: (N, D, H, W, C)\n        # We will generate data in JAX format.\n        return nn.ConvTranspose(features=out_channels, kernel_size=(kernel_size, kernel_size, kernel_size), use_bias=False)(x)\n\n# Test code\nbatch_size = 8\ndepth = 64\nheight = 64\nwidth = 64\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, depth, height, width, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)\n", "student_template": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\nin_channels = 48\nout_channels = 48\nkernel_size = 3\n\nclass Model(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        \"\"\"\n        Complete the conv transposed D square input square kernel forward pass.\n        \"\"\"\n        # TODO: Replace this placeholder with your implementation.\n        # Helpful hints:\n        # - Review the tensors emitted by `get_inputs` so your function signature stays compatible.\n        # - Keep every computation inside JAX/Flax; avoid NumPy on the host to stay TPU-ready.\n        # - Instantiate and call `nn.Conv` with the channel counts shown in the scaffold.\n        # - Instantiate `nn.ConvTranspose` with the provided strides, kernel size, and channel config.\n        # - Reuse the provided PRNG splitting strategy so shapes stay reproducible.\n        raise NotImplementedError('Implement the conv transposed D square input square kernel logic')\n\n# Test code\nbatch_size = 8\ndepth = 64\nheight = 64\nwidth = 64\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, depth, height, width, in_channels))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    Returns the dummy input needed to initialize the model.\n    \"\"\"\n    return get_inputs(key)", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **conv transposed D square input square kernel** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "192882fc-99f5-4fd9-994d-29880762560f", "title": "This code implements  LogSoftmax", "difficulty": "easy", "description": "### Challenge Brief\nSimple model that performs a LogSoftmax activation. Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\nbatch_size = 4096\ndim = 393216\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(x):\n    \"\"\"\n    Simple model that performs a LogSoftmax activation.\n    \"\"\"\n    return nn.log_softmax(x, axis=1)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the input tensor [x].\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []\n", "student_template": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(x):\n    \"\"\"\n    Simple model that performs a LogSoftmax activation.\n    \"\"\"\n    # TODO: Replace this placeholder with your implementation.\n    # Helpful hints:\n    # - Review the tensors emitted by `get_inputs` so your function signature stays compatible.\n    # - Keep every computation inside JAX/Flax; avoid NumPy on the host to stay TPU-ready.\n    # - Apply `nn.log_softmax` across the class dimension.\n    # - Reuse the provided PRNG splitting strategy so shapes stay reproducible.\n    raise NotImplementedError('Implement the LogSoftmax logic')\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the input tensor [x].\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **LogSoftmax** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "ec9e3aff-cfa7-41dd-a4a7-72d65abc8d1d", "title": "This code implements  LeakyReLU", "difficulty": "easy", "description": "### Challenge Brief\nSimple model that performs a LeakyReLU activation. Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\nbatch_size = 4096\ndim = 393216\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(x):\n    \"\"\"\n    Simple model that performs a LeakyReLU activation.\n    \"\"\"\n    return nn.leaky_relu(x, negative_slope=0.01)\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the input tensor [x].\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []\n", "student_template": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(x):\n    \"\"\"\n    Simple model that performs a LeakyReLU activation.\n    \"\"\"\n    # TODO: Replace this placeholder with your implementation.\n    # Helpful hints:\n    # - Review the tensors emitted by `get_inputs` so your function signature stays compatible.\n    # - Keep every computation inside JAX/Flax; avoid NumPy on the host to stay TPU-ready.\n    # - Honor the negative slope used by `nn.leaky_relu` in the reference.\n    # - Reuse the provided PRNG splitting strategy so shapes stay reproducible.\n    raise NotImplementedError('Implement the LeakyReLU logic')\n\nbatch_size = 4096\ndim = 393216\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input tensor.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the input tensor [x].\n    \"\"\"\n    x = jax.random.normal(key, (batch_size, dim))\n    return [x]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **LeakyReLU** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "1b5d8c99-1744-4d39-b3a3-abebdfbf5cb0", "title": "This code implements  Matrix scalar multiplication", "difficulty": "easy", "description": "### Challenge Brief\nSimple model that performs a matrix-scalar multiplication (C = A * s) Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\nM = 16384 * 4\nN = 4096 * 4\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(A, s):\n    \"\"\"\n    Simple model that performs a matrix-scalar multiplication (C = A * s)\n    \"\"\"\n    return A * s\n\nM = 16384 * 4\nN = 4096 * 4\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input matrix and a scalar.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the input matrix and scalar [A, s].\n    \"\"\"\n    A = jax.random.normal(key, (M, N))\n    s = 3.14\n    return [A, s]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []\n", "student_template": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(A, s):\n    \"\"\"\n    Simple model that performs a matrix-scalar multiplication (C = A * s)\n    \"\"\"\n    # TODO: Replace this placeholder with your implementation.\n    # Helpful hints:\n    # - Review the tensors emitted by `get_inputs` so your function signature stays compatible.\n    # - Keep every computation inside JAX/Flax; avoid NumPy on the host to stay TPU-ready.\n    # - Reuse the provided PRNG splitting strategy so shapes stay reproducible.\n    raise NotImplementedError('Implement the Matrix scalar multiplication logic')\n\nM = 16384 * 4\nN = 4096 * 4\n\ndef get_inputs(key):\n    \"\"\"\n    Generates a random input matrix and a scalar.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing the input matrix and scalar [A, s].\n    \"\"\"\n    A = jax.random.normal(key, (M, N))\n    s = 3.14\n    return [A, s]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **Matrix scalar multiplication** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "90ce8d43-5516-48eb-999e-bfd36cf113f6", "title": "This code implements  Matmul for lower triangular matrices", "difficulty": "easy", "description": "### Challenge Brief\nSimple model that performs a matrix multiplication (C = A * B) where A and B are lower triangular matrices. Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\nM = 4096\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(A, B):\n    \"\"\"\n    Simple model that performs a matrix multiplication (C = A * B) where A and B are lower triangular matrices. \n    \"\"\"\n    return jnp.tril(jnp.matmul(A, B))\n\nM = 4096\n\ndef get_inputs(key):\n    \"\"\"\n    Generates lower triangular matrices for testing.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing two lower triangular matrices of shape (M, M).\n    \"\"\"\n    key_a, key_b = jax.random.split(key)\n    A = jax.random.normal(key_a, (M, M))\n    B = jax.random.normal(key_b, (M, M))\n    A = jnp.tril(A)\n    B = jnp.tril(B)\n    return [A, B]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []\n", "student_template": "import jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\n\ndef model(A, B):\n    \"\"\"\n    Simple model that performs a matrix multiplication (C = A * B) where A and B are lower triangular matrices.\n    \"\"\"\n    # TODO: Replace this placeholder with your implementation.\n    # Helpful hints:\n    # - Review the tensors emitted by `get_inputs` so your function signature stays compatible.\n    # - Keep every computation inside JAX/Flax; avoid NumPy on the host to stay TPU-ready.\n    # - Use `jnp.matmul` (or the `@` operator) to perform the core multiply without leaving JAX.\n    # - Use `jnp.tril` to zero-out everything above the diagonal before continuing.\n    # - Reuse the provided PRNG splitting strategy so shapes stay reproducible.\n    raise NotImplementedError('Implement the Matmul for lower triangular matrices logic')\n\nM = 4096\n\ndef get_inputs(key):\n    \"\"\"\n    Generates lower triangular matrices for testing.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: A list containing two lower triangular matrices of shape (M, M).\n    \"\"\"\n    key_a, key_b = jax.random.split(key)\n    A = jax.random.normal(key_a, (M, M))\n    B = jax.random.normal(key_b, (M, M))\n    A = jnp.tril(A)\n    B = jnp.tril(B)\n    return [A, B]\n\ndef get_init_inputs(key):\n    \"\"\"\n    No special initialization inputs needed.\n\n    Args:\n        key (jax.random.PRNGKey): JAX random key.\n\n    Returns:\n        list: An empty list.\n    \"\"\"\n    return []", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **Matmul for lower triangular matrices** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "6c6f5c6f-99a5-4c3a-a028-89c5e1424960", "title": "Stable Softmax With JAX", "difficulty": "medium", "description": "### Challenge Brief\nFinish implementing **Stable Softmax With JAX** on TPU hardware. Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef stable_softmax(x: jnp.ndarray) -> jnp.ndarray:\n    \"\"\"Return the numerically stable softmax of x.\"\"\"\n    raise NotImplementedError(\"Implement me using JAX primitives only\")\n", "student_template": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef stable_softmax(x: jnp.ndarray) -> jnp.ndarray:\n    \"\"\"Return the numerically stable softmax of x.\"\"\"\n    raise NotImplementedError(\"Implement me using JAX primitives only\")\n", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **Stable Softmax With JAX** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "0c5feaba-c818-4ac1-8bea-e89c14a2f0cf", "title": "JIT-Tuned Training Step", "difficulty": "easy", "description": "### Challenge Brief\nFinish implementing **JIT-Tuned Training Step** on TPU hardware. Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Reference Configuration\n```python\nArray = jax.Array\n```\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\nfrom typing import Tuple\n\nArray = jax.Array\n\ndef sigmoid_cross_entropy(logits: Array, labels: Array) -> Array:\n    return -labels * jax.nn.log_sigmoid(logits) - (1.0 - labels) * jax.nn.log_sigmoid(-logits)\n\n@jax.jit\ndef train_step(params: Tuple[Array, Array], batch: Tuple[Array, Array], learning_rate: float) -> Tuple[Tuple[Array, Array], Array]:\n    \"\"\"Run one training step and return (new_params, loss).\"\"\"\n\n    raise NotImplementedError(\"Use jax.grad to differentiate the loss and update params\")\n", "student_template": "import jax\nimport jax.numpy as jnp\nfrom typing import Tuple\n\nArray = jax.Array\n\ndef sigmoid_cross_entropy(logits: Array, labels: Array) -> Array:\n    return -labels * jax.nn.log_sigmoid(logits) - (1.0 - labels) * jax.nn.log_sigmoid(-logits)\n\n@jax.jit\ndef train_step(params: Tuple[Array, Array], batch: Tuple[Array, Array], learning_rate: float) -> Tuple[Tuple[Array, Array], Array]:\n    \"\"\"Run one training step and return (new_params, loss).\"\"\"\n\n    raise NotImplementedError(\"Use jax.grad to differentiate the loss and update params\")\n", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **JIT-Tuned Training Step** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
{"id": "54cfa124-f5cb-4bd6-9d51-2460ea5a12d3", "title": "Custom Gradient RMSNorm", "difficulty": "medium", "description": "### Challenge Brief\nFinish implementing **Custom Gradient RMSNorm** on TPU hardware. Ensure the TPU worker can execute it end-to-end.\n\n### Context\nStudy the helper functions and constants so you can document tensor ranks, dtypes, and memory impact before coding. Capture those notes in Markdown for future readers.\n\n### Subtask – Enrich the README\n- [ ] Rewrite this challenge summary in your own Markdown words.\n- [ ] Add bullet points covering tensor shapes, special ops (transposes, pooling, norms), and TPU gotchas.\n- [ ] Share it with a peer and update it based on their feedback.\n\n### Requirements\n- Implement the missing logic using the provided function/module signatures.\n- Keep all math in JAX/Flax so it can compile on TPU.\n- Accept the tensors from `get_inputs` without changing their contract.\n- Return only the computed outputs—no prints or side effects.\n\n### Tips\n- Skim the `student_template` hints before you start coding.\n- Validate your solution with smaller shapes locally, then scale back up.\n- Note any performance considerations directly in Markdown so reviewers understand your approach.", "code": "import jax\nimport jax.numpy as jnp\n\n@jax.custom_vjp\ndef rmsnorm(x: jax.Array, scale: jax.Array | None = None, eps: float = 1e-6) -> jax.Array:\n    \"\"\"Compute RMSNorm over the last dimension.\"\"\"\n    raise NotImplementedError(\"Return the normalized tensor (and apply scale if provided)\")\n\n# TODO: define forward/backward functions with @rmsnorm.defvjp\n", "student_template": "import jax\nimport jax.numpy as jnp\n\n@jax.custom_vjp\ndef rmsnorm(x: jax.Array, scale: jax.Array | None = None, eps: float = 1e-6) -> jax.Array:\n    \"\"\"Compute RMSNorm over the last dimension.\"\"\"\n    raise NotImplementedError(\"Return the normalized tensor (and apply scale if provided)\")\n\n# TODO: define forward/backward functions with @rmsnorm.defvjp\n", "subtasks": [{"title": "Polish the Markdown brief", "description": "Summarize the **Custom Gradient RMSNorm** computation in your own Markdown doc, including tensor shapes, hyperparameters, and TPU-specific considerations. Have a teammate review it before marking this complete."}]}
